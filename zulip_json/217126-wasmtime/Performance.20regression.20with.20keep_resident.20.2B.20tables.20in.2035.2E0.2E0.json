[
    {
        "content": "<p>I just finished debugging a performance regression in Spin and wanted to write it down here in case anyone else is affected by this. We were seeing a performance regression when Wasmtime 34 was updated to Wasmtime 35. After various layers of bisection I found that <a href=\"https://github.com/bytecodealliance/wasmtime/pull/10388\">https://github.com/bytecodealliance/wasmtime/pull/10388</a> was the culprit.</p>\n<p>The stack-switching PR was disabled for Wasmtime 34 despite merging just before the branch (I reverted it manually to de-risk Wasmtime 34). The PR, however, lives in Wasmtime 35 and 36 and main today. After puzzling over how an off-by-default feature would affect performance so drastically I discovered that the PR inadvertently changed the behavior of <code>TablePool::reset_table_pages_to_zero</code> where previously only the table's size was reset and afterwards the entire table slot was reset. A calculation of <code>table.size() * mem::size_of::&lt;*mut u8&gt;()</code> was changed to <code>self.data_size(table.element_type())</code> where the latter is the size of the whole slot. A normal bug to happen so no one's at fault of course.</p>\n<p>This meant, though, that when combined with <code>*_keep_resident</code> options it means that tables could have a way higher memset amount afterwards than before (for the same-size tables too). This ended up being the source of our performance regression.</p>\n<p>The reason I'm talking about this here instead of on GitHub is that this is inadvertently already fixed. I ended up fixing this behavior in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11341\">https://github.com/bytecodealliance/wasmtime/pull/11341</a> mistakenly assuming that the table pool allocator had always reset the entire slot instead of just the table itself. Basically I didn't realize that the behavior I was changing had itself changed recently with the merging of stack-switching. That PR did not make its way into Wasmtime 35 but it has made its way into Wasmtime 36.</p>\n<p>So tl;dr; if you use <code>*_keep_resident</code> and see a performance regression on Wasmtime 35 but not 34 or 36 this may be why.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/wasmtime/pull/10388\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/ae156b502d99e70b41799fcda1a5f96c17e636e9/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f363862623431376134323031303531303261373834336437646131353664343865373535643962343766656164323962333463636331666661626438396332632f62797465636f6465616c6c69616e63652f7761736d74696d652f70756c6c2f3130333838&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/wasmtime/pull/10388\" title=\"Stack switching: Infrastructure and runtime support by frank-emrich · Pull Request #10388 · bytecodealliance/wasmtime\">Stack switching: Infrastructure and runtime support by frank-emrich · Pull Request #10388 · bytecodealliance/wasmtime</a></div><div class=\"message_embed_description\">This PR is part of a series that adds support for the Wasm stack switching proposal. The explainer document for the proposal is here. There&#39;s a tracking issue describing the overall progress an...</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/wasmtime/pull/11341\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/10b832be8dd81af42279e0f142440e039016908a/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f383563626466666432336465336437626439663064656330363835646139323635366539373661623164396265623537646633366561303562343161316562332f62797465636f6465616c6c69616e63652f7761736d74696d652f70756c6c2f3131333431&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/wasmtime/pull/11341\" title=\"Reset fewer bytes when resetting tables by alexcrichton · Pull Request #11341 · bytecodealliance/wasmtime\">Reset fewer bytes when resetting tables by alexcrichton · Pull Request #11341 · bytecodealliance/wasmtime</a></div><div class=\"message_embed_description\">This commit changes the resetting of tables back to all-null in TablePool::reset_table_pages_to_zero. Previously the full capacity of the table was reset back to zero, depending on the configuratio...</div></div></div>",
        "id": 535556893,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1755800632
    },
    {
        "content": "<p>Interesting, thanks for finding this and for the writeup; that's definitely an error I made while working to address feedback on the stack-switching runtime changes.</p>",
        "id": 536668438,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1756407859
    },
    {
        "content": "<p>That likely explains this profile I've seen on Wasmtime 35 with <code>keep_resident</code> options set <a href=\"https://profiler.firefox.com/public/8zfwkxghrmkz4d4bzd6zb9famgrx76kdw617bw0/flame-graph/?globalTrackOrder=102&amp;hiddenGlobalTracks=02&amp;symbolServer=http%3A%2F%2F127.0.0.1%3A3333%2F3nc2077n82dr7b772r2b9l8vm41mhn9anasg47j&amp;thread=2whswxpxrwzmzowAlAnwCiCkwDhDjwFeFgwGdGfwIb&amp;transforms=f-combined-gwkx8wxbxdxeyuyvz3z4&amp;v=11\">https://profiler.firefox.com/public/8zfwkxghrmkz4d4bzd6zb9famgrx76kdw617bw0/flame-graph/?globalTrackOrder=102&amp;hiddenGlobalTracks=02&amp;symbolServer=http%3A%2F%2F127.0.0.1%3A3333%2F3nc2077n82dr7b772r2b9l8vm41mhn9anasg47j&amp;thread=2whswxpxrwzmzowAlAnwCiCkwDhDjwFeFgwGdGfwIb&amp;transforms=f-combined-gwkx8wxbxdxeyuyvz3z4&amp;v=11</a></p>\n<p>I've removed these options to work around it, but maybe I should revisit them in 36</p>",
        "id": 536804579,
        "sender_full_name": "Roman Volosatovs",
        "timestamp": 1756478739
    },
    {
        "content": "<p>Your profile Roman shows most of the memset from <code>deallocate_memories</code> which shouldn't have changed between 34/35/36, so that may be something else?</p>",
        "id": 536804967,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1756478876
    },
    {
        "content": "<p>In that embedding the memory size is actually static across all modules, and it's pretty big, so I just assumed that it's simply too big for the feature. <code>madvise</code> performed way better in my testing.<br>\nI did see the table deallocation also incur significant cost, but I've since lost that profile - removing the <code>keep_resident</code> options fixed both issues for me that time.</p>\n<p>for reference, here's the complete set of config I landed upon in the end: <a href=\"https://github.com/near/nearcore/blob/359902578a29d4542fb0d816c9cee2a45341d4a0/runtime/near-vm-runner/src/wasmtime_runner/mod.rs#L353-L414\">https://github.com/near/nearcore/blob/359902578a29d4542fb0d816c9cee2a45341d4a0/runtime/near-vm-runner/src/wasmtime_runner/mod.rs#L353-L414</a></p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/near/nearcore/blob/359902578a29d4542fb0d816c9cee2a45341d4a0/runtime/near-vm-runner/src/wasmtime_runner/mod.rs#L353-L414\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/95141b1983b26291542bb02ad4acfe5bd2097a39/68747470733a2f2f7265706f7369746f72792d696d616765732e67697468756275736572636f6e74656e742e636f6d2f3135313333313933382f62306363386638302d306434362d313165622d393435302d376435643465313865353466&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/near/nearcore/blob/359902578a29d4542fb0d816c9cee2a45341d4a0/runtime/near-vm-runner/src/wasmtime_runner/mod.rs#L353-L414\" title=\"nearcore/runtime/near-vm-runner/src/wasmtime_runner/mod.rs at 359902578a29d4542fb0d816c9cee2a45341d4a0 · near/nearcore\">nearcore/runtime/near-vm-runner/src/wasmtime_runner/mod.rs at 359902578a29d4542fb0d816c9cee2a45341d4a0 · near/nearcore</a></div><div class=\"message_embed_description\">Reference client for NEAR Protocol. Contribute to near/nearcore development by creating an account on GitHub.</div></div></div>",
        "id": 536806163,
        "sender_full_name": "Roman Volosatovs",
        "timestamp": 1756479244
    },
    {
        "content": "<p>at one point I've updated to 36 and it gave some performance benefits</p>",
        "id": 536806397,
        "sender_full_name": "Roman Volosatovs",
        "timestamp": 1756479309
    },
    {
        "content": "<p>I would definitely expect that for certain workloads keep_resident can make things worse, with or without the bug (though worse with the 35 bug).  This is true with the pagemap optimizations as well.  The biggest penalty moves a bit with madvise to arise during page faults, but if there's very few dirtied pages then both the madvise and the page faults can be, in aggregate, pretty expensive.</p>\n<p>I did some comparisons of those tradeoffs in comments I added later to <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11372\">https://github.com/bytecodealliance/wasmtime/pull/11372</a>.  What I don't show there is any comparison of a single huge madvise compared against a pagemap scan + madvise.  I felt it reached the point where just trying to bench real workloads made more sense.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/wasmtime/pull/11372\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/47ff227b8a4c951b7c35239dfb1056a3f319ac14/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f623361396135626161333030633664386266373631313966373537356632643933376638636461653565616333373437633164316162393433353062626330322f62797465636f6465616c6c69616e63652f7761736d74696d652f70756c6c2f3131333732&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/wasmtime/pull/11372\" title=\"Add support for the Linux PAGEMAP_SCAN ioctl by alexcrichton · Pull Request #11372 · bytecodealliance/wasmtime\">Add support for the Linux PAGEMAP_SCAN ioctl by alexcrichton · Pull Request #11372 · bytecodealliance/wasmtime</a></div><div class=\"message_embed_description\">This series of commits is the brainchild of @tschneidereit who, in his spare time, reads Linux kernel documentation and finds random ioctls. Specifically @tschneidereit discovered the PAGEMAP_SCAN ...</div></div></div>",
        "id": 536829239,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1756486735
    }
]