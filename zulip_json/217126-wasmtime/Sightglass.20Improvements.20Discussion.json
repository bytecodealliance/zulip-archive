[
    {
        "content": "<p>CI for sightglass appears to be broken due to using out of date actions, so I'm working on a basic update to improve that.  I'm wondering if other contributors have thoughts on other critical work or workflow improvements to sightglass that we could prioritize.</p>\n<p>I'm testing a fork now that fixes CI but it has been running for over 3 hours :/ <a href=\"https://github.com/posborne/sightglass/pull/1\">https://github.com/posborne/sightglass/pull/1</a></p>\n<p>Some ideas that come to mind to encourage contributions in the near term are probably to:</p>\n<ol>\n<li>Reduce CI times through setting up parallel jobs to run the <code>all</code> suite.</li>\n<li>Possibly remove prohibitively long-running benchmarks from PR requirements or otherwise tune this (libsodium seems to be the culprit here).</li>\n<li>Introduce nightly CI to do full benchmark suite if we remove some from normal PR flow.</li>\n</ol>\n<p>Ultimately, just using the stock gha runners are probably not ideal for benchmarking but near term I think we can probably improve a lot without tackling that.</p>\n<p>Something along the lines of <a href=\"https://github.com/bytecodealliance/sightglass/issues/93\">https://github.com/bytecodealliance/sightglass/issues/93</a> looks great to me, but it may be useful to even target something easier to deploy or enable local experimentation and comparison of results.  I'm thinking just something that would be able to take the structured output of runs from a few different configurations (for nightly we could just do <code>main</code> of wasmtime plus a few tagged versions) and outputs results showing percentage differences.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/posborne/sightglass/pull/1\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/98658078ba1b71d3b1687219ecdee7d57e8efd81/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f326539616538653063363335613761353830363265353265383862643866363433333431336230633234393565333133373238613337313963303933636665312f706f73626f726e652f7369676874676c6173732f70756c6c2f31&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/posborne/sightglass/pull/1\" title=\"ci: remove checkout@v2 and actions-rs/toolchain by posborne · Pull Request #1 · posborne/sightglass\">ci: remove checkout@v2 and actions-rs/toolchain by posborne · Pull Request #1 · posborne/sightglass</a></div><div class=\"message_embed_description\">Also changes to just use stable rust in more cases where nightly features don't seem to be required.\n\nTrial CI against the fork first to get things working.</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/sightglass/issues/93\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/b5c9415e45e427623a270ab49556543d8154ba31/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f646330386638383764303731323939633332363466333431303134383637373036353565363061633238623963303933383333343365386639393738323664662f62797465636f6465616c6c69616e63652f7369676874676c6173732f6973737565732f3933&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/sightglass/issues/93\" title=\"sightglass-next: implement sightglass-server · Issue #93 · bytecodealliance/sightglass\">sightglass-next: implement sightglass-server · Issue #93 · bytecodealliance/sightglass</a></div><div class=\"message_embed_description\">In order to report performance results based on PRs, we talked about implementing an HTTP server (e.g. in crates/server) that would: listen for incoming POST requests that contain JSON with the PR ...</div></div></div>",
        "id": 511502284,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744316775
    },
    {
        "content": "<p>Having a nightly runner of some sort that provides trends over time would be great, and we've talked about that / wanted that for five years or so now...</p>\n<p>At one point we (BA) were renting a dedicated x86-64 machine on Hetzner because this seemed \"coming soon\" but we gave that up a year or so ago. I imagine once we have a workable runner it wouldn't be a huge deal to get this going again. The main concern at the time was security -- we didn't want to allow PRs from arbitrary people to spawn execution and there was a bit of indirection around that IIRC (private repo, ...) A nightly run of <code>main</code> solves that problem at least.</p>",
        "id": 511502882,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744317023
    },
    {
        "content": "<p>I can't comment much on the topic of CI for the repo itself, but on the topic of continually running benchmarks it's true that security was historically a concern, but bjorn3 showed a way that we can solve this nowadays. That means that it's now possible to actually hook up a custom runner to the public wasmtime repo in a way that doesn't let contributors run arbitrary code on it. </p>\n<p>In that sense I agree it'd be awesome to get this up and running, but one of the historic blockers as well has been someone to help set up and manage the infrastructure. Even if it's \"mostly just github actions\" it's still a fair amount to orchestrate</p>",
        "id": 511504304,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1744317639
    },
    {
        "content": "<p>Yeah, I think there's a lot of improvements we can make without that.  Near term, I think with purely gh-runners we can get useful enough results by ensuring that a single benchmark for different configurations (probably just different wasmtime versions) is run on the same runner. </p>\n<p>This is still far from controlled, but I would take mildly noisy results over no results -- that impact can probably be reduced some if we can incorporate perf counters info in the nightly results, etc.</p>\n<p>I think near term wins would be:</p>\n<ol>\n<li>Make running a comparative benchmark of a branch on a dev/maintainers machine of wasmtime vs. baseline easier.</li>\n<li>Provide something relatively reliable that we can look at to see if we have a major perf improvement/regression on main compared to some previous release.</li>\n</ol>",
        "id": 511505511,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744318117
    },
    {
        "content": "<p><a href=\"https://github.com/bytecodealliance/sightglass/pull/284\">https://github.com/bytecodealliance/sightglass/pull/284</a></p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/sightglass/pull/284\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/3d1e7fc5b3bb9f64eea7a9a77737a48ecb3bbccf/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f653639376132313930333063633235343739373663356532366135353938656631313031346632373738383931386262346137346136636639316164643033362f62797465636f6465616c6c69616e63652f7369676874676c6173732f70756c6c2f323834&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/sightglass/pull/284\" title=\"Grab Bag of CI Enhancements by posborne · Pull Request #284 · bytecodealliance/sightglass\">Grab Bag of CI Enhancements by posborne · Pull Request #284 · bytecodealliance/sightglass</a></div><div class=\"message_embed_description\">This first set of changes includes the following:\n\nRemoves outdated github actions recipes that are no longer functioning.\nUpdates the benchmark flow to complete much more quickly by:\n\nDon&#39;t tr...</div></div></div>",
        "id": 512187993,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744671571
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"254110\">@Andrew Brown</span> Copying you on this for additional context on the PR in case you didn't see this thread; hoping to at least have a rough cut of nightly with some kind of easily consumed output later this week.</p>",
        "id": 512363254,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744733973
    },
    {
        "content": "<p>Ah, cool! Glad to see you're working on this.</p>",
        "id": 512370851,
        "sender_full_name": "Andrew Brown",
        "timestamp": 1744736089
    },
    {
        "content": "<p>Some more context:</p>\n<ul>\n<li>I did create an entire Kibana-based visualization server based on all of this some years ago: <a href=\"https://github.com/abrown/sightglass/tree/ui/ui\">https://github.com/abrown/sightglass/tree/ui/ui</a>. There were different ways people wanted to view the data and that Kibana approach allowed for creating all kinds of charts from data stored in Elasticsearch. I eventually abandoned that not only because of the issues mentioned above re: GitHub triggers but also because it became clear to me that maintaining all of this would be too much work. So my thoughts on any UI we build is to keep it simple!</li>\n<li>Most of the metadata collection tools are upstreamed (as you may have seen) but will likely be biased towards creating the JSON documents that ES could easily use.</li>\n<li>One thing we've discussed in the past is to use Sightglass to measure instruction counts, e.g., to avoid noisiness; might be worth a look, but I would say noisy results is also a good start.</li>\n<li>Recently I looked into integrating Sightglass benchmarking of Wasmtime into <a href=\"https://codspeed.io\">https://codspeed.io</a>. There was some reticence in the Cranelift meeting to this approach due to vendor lock-in, e.g., but the advantage of something like this is that they are incentivized to maintain things!</li>\n</ul>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/abrown/sightglass/tree/ui/ui\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/cb57959aced76de8e453361d1fbf038922924d26/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f383531363337366530373232313031613531386366316437323563643033343461323231383635376338613931653562366663613635336638373735383537312f6162726f776e2f7369676874676c617373&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/abrown/sightglass/tree/ui/ui\" title=\"sightglass/ui at ui · abrown/sightglass\">sightglass/ui at ui · abrown/sightglass</a></div><div class=\"message_embed_description\">A benchmark suite and tool to compare different implementations of the same primitives. - abrown/sightglass</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://codspeed.io\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/5868a6acf2b8b7db3c9964f0573eba30ea977ff2/68747470733a2f2f636f6473706565642e696f2f6f675f696d6167652e706e67&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://codspeed.io\" title=\"CodSpeed: Optimize Performance, Eliminate Regressions\">CodSpeed: Optimize Performance, Eliminate Regressions</a></div><div class=\"message_embed_description\">CodSpeed integrates into dev and CI workflows to measure performance, detect regressions, and enable actionable optimizations.</div></div></div>",
        "id": 512373502,
        "sender_full_name": "Andrew Brown",
        "timestamp": 1744736921
    },
    {
        "content": "<p>Excellent, thanks for the context.  I share the reluctance to add in a 3rd party tool or service that requires upkeep.  I think what I'll target to start is something that is static and targets gh-pages.</p>\n<p>Something like <a href=\"https://github.com/marketplace/actions/continuous-benchmark\">https://github.com/marketplace/actions/continuous-benchmark</a> seems like it could be promising; it supports ingesting a \"custom\" json format, which i should be able to target by doing a transform on the benchmark outputs.</p>\n<p>If setup on a scheduled run, this should allow us to:</p>\n<ul>\n<li>Get some charted data on benchmark performance over time (possibly matrixed with a few interesting variants).</li>\n<li>Setup notifications for significant performance regressions on wasmtime <code>main</code> (depending on SnR b/w runs).</li>\n</ul>\n<p>I'll do that work in my fork for for a bit and might request an early review there before upstreaming as permissions for CI are easier to manage in that way until ready to go.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/marketplace/actions/continuous-benchmark\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/7e271e439f19e8e3798449f8548650e03bb87879/68747470733a2f2f617661746172732e67697468756275736572636f6e74656e742e636f6d2f752f39313736393433323f733d34303026763d34&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/marketplace/actions/continuous-benchmark\" title=\"Continuous Benchmark - GitHub Marketplace\">Continuous Benchmark - GitHub Marketplace</a></div><div class=\"message_embed_description\">Continuous Benchmark using GitHub pages as dash board for keeping performance</div></div></div>",
        "id": 512384849,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744740458
    },
    {
        "content": "<p>At least in my early runs here, I'm seeing quite a range of cycle counts across iterations of benchmark runs for several of the tests.  Does this look normal?  For example, <a href=\"https://github.com/posborne/sightglass/actions/runs/14499832348/job/40677009588\">https://github.com/posborne/sightglass/actions/runs/14499832348/job/40677009588</a> (look at the \"Output Results\" JSON).</p>\n<p>This is after a transform I'm playing with but I've verified the \"raw\" JSON results show similar discrepancies.  I just kicked off another run that removes parallel processes to see if that reduces the variability but something seems suspect.  Doing more iterations and/or taking a median over a mean and rejecting outliers.  What seems suspicious is that the variance seems to be much more pronounced for some benchmarks (though, possibly just ones that are shorter).</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/posborne/sightglass/actions/runs/14499832348/job/40677009588\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/5fb93c332b7edd93f5461deb078da61079980c1e/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f333531643936363866636631306335646231663033613766306364366330623836336433323064393636376430346663396635393464616536656335323634362f706f73626f726e652f7369676874676c617373&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/posborne/sightglass/actions/runs/14499832348/job/40677009588\" title=\"Periodic Benchmark Run · posborne/sightglass@bc4f149\">Periodic Benchmark Run · posborne/sightglass@bc4f149</a></div><div class=\"message_embed_description\">A benchmark suite and tool to compare different implementations of the same primitives. - Periodic Benchmark Run · posborne/sightglass@bc4f149</div></div></div>",
        "id": 512679405,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744834868
    },
    {
        "content": "<p>I'm going to see if perf counters can work on runners, though I'm not terribly optimistic.</p>",
        "id": 512679934,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744835089
    },
    {
        "content": "<p>Do you have any platform information (/proc/cpuinfo, etc)? It seems likely to me that the GitHub Runner fleet is heterogeneous and there's no reason to expect that cycle count would be consistent across separate runs because of that</p>",
        "id": 512680015,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744835120
    },
    {
        "content": "<p>I can probably get that info, but in this case the samples are iterations from the same run on the same node in the same process back-to-back (assuming my understanding of the tool is correct).  If the results in that scenario aren't even (reasonably) consistent it puts doubts on a lot of measurements.</p>\n<p>With that in mind, I was seeing similar large deltas when testing locally and looking at the raw results as well (in that case on an m4 macbook).</p>",
        "id": 512681174,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744835555
    },
    {
        "content": "<p>Some of the tests will run in parallel on different nodes but I'm not comparing results (at this point) across runners at all.</p>",
        "id": 512681290,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744835605
    },
    {
        "content": "<p>ah sorry, I missed that. High variation even when running locally is kind of surprising, though we've done work (or specifically <span class=\"user-mention\" data-user-id=\"504918\">@Jamey Sharp</span> did) to document how to get a very low-noise measurement. It involved pinning one core of his laptop at its minimum frequency, disabling sleep/power-saving features I think, and modifying systemd config to keep all other processes off the core. Maybe resteering interrupts too?</p>",
        "id": 512681471,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744835681
    },
    {
        "content": "<p>All this to say, instruction counts may be good to have as well!</p>",
        "id": 512681561,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744835719
    },
    {
        "content": "<p>Yeah, that sounds promising.  This isn't all tests, but is the raw data from a gh-runner for an eigth of the tests.  It does seem like a consistent theme is that the first iteration is measured as slower.  This seems like it could be expected to a small degree (and I would be fine with just discarding this measurement) but I'm seeing a lot that are 30-40% which seems quite big.</p>\n<p>The data here only had 4 iterations per benchmark (will be increasing once stable) but gives a little picture.</p>\n<p><a href=\"/user_uploads/15107/VkQiUcjNd7fr9z00qZIHDAcT/benchmark-results-2.json\">benchmark-results 2.json</a></p>",
        "id": 512682321,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744836007
    },
    {
        "content": "<p>Wow, 30-40% is a huge swing. I guess it seems likely this could be a noisy-neighbor problem too -- runners have no reason to be tuned for performance consistency, rather they would want to pack jobs as tightly as possible and provide opportunistic performance when available</p>",
        "id": 512682657,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744836167
    },
    {
        "content": "<p>I've got to look at more runs to gain confidence on a few pieces.  In particular:</p>\n<ol>\n<li>Are there certain benchmarks that are more prone to seeing varied results (beyond just that shorter-lived ones can expect to see larger pct variance) or is it across all.</li>\n<li>Do we see the same variance looking at wall time; could there be some issues with the \"cycles\" counting (haven't looked at the impl).</li>\n</ol>\n<p><span aria-label=\"sad\" class=\"emoji emoji-2639\" role=\"img\" title=\"sad\">:sad:</span> No perf-events in gh runners, it would appear (unsuprisingly): <code>Unable to create event group; try setting /proc/sys/kernel/perf_event_paranoid to 2 or below?: Os { code: 1, kind: PermissionDenied, message: \"Operation not permitted\" }</code></p>",
        "id": 512684271,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744836866
    },
    {
        "content": "<p>yeah that frequently comes down to the hypervisor not giving access to the CPU's performance counters, in addition to the linux kernel running in the guest not supporting it as a knock-on effect</p>",
        "id": 512685268,
        "sender_full_name": "Pat Hickey",
        "timestamp": 1744837291
    },
    {
        "content": "<p>back in the early days of cretonne/lucet i expensed an Intel Nuc and we just hid it away in the server room somewhere so we had a place to somewhat reilably run benchmarks</p>",
        "id": 512685472,
        "sender_full_name": "Pat Hickey",
        "timestamp": 1744837367
    },
    {
        "content": "<p>maybe its still there! lol. but usually if theres been a power outage it requires someone to go into the server room in the fastly sf office and hit a button, and th</p>",
        "id": 512685548,
        "sender_full_name": "Pat Hickey",
        "timestamp": 1744837398
    },
    {
        "content": "<blockquote>\n<p>stopped being possible in early 2020 for some reason</p>\n</blockquote>\n<p>I was writing Cranelift's aarch64 backend in Feb 2020 and left my RPi4 on my desk in Mozilla's Mountain View office, plugged into power and ethernet, and went home one Friday, then unexpectedly found myself ssh'ing in from Monday onward... that little guy kept going without a single complaint until around June at which point it stopped responding to pings. That sure was a weird period of time. (Thank goodness for actual datacenter control plane + out-of-band stuff.)</p>",
        "id": 512688887,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744838157
    },
    {
        "content": "<p>I may have to hunt that down; I've got some reqs out to get something like that going for my regular development use but a standalone node would be useful for getting consistent benchmarks (especially if configured to do stuff to tune for reproducible results with frequency scaling, etc.).</p>\n<p>Reviewing the <code>precision</code> crate used for cycle measurement, I don't see a huge issue with the number it is grabbing (uses the <code>rdtscp</code> instruction on x64) and it should help with differences in frequency scaling but isn't a silver bullet, obviously.</p>",
        "id": 512688995,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744838171
    },
    {
        "content": "<p>We did rent a machine on Hetzner for a few years, and I suspect it wouldn't be hard to go back to doing that, though we'd need to arrange budget for it via the TSC etc</p>",
        "id": 512689183,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744838210
    },
    {
        "content": "<p>(for the purposes of benchmarking, though it was only sporadically used, hence the \"used to\")</p>",
        "id": 512689253,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1744838224
    },
    {
        "content": "<p>Yeah, for running benchmarks as a one-off I can definitely reserve lab nodes within fastly, etc. I think I was hoping gh runners would end up being consistent enough to be used for continuous benchmarking as managing hardware for use by the open source project seems like a bit of a pain.  I guess that would also brings us back around to considering  something like <a href=\"https://codspeed.io\">https://codspeed.io</a> as Andrew mentioned.</p>\n<p>I think I'll deliver something on gh as a POC regardless and we can decide how meaningful or meaningless the results are; we can then experiment with hosted runners and go from there if it makes sense.  Going that direction may be desired anyhow if/when we want coverage of more ISAs.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://codspeed.io\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/5868a6acf2b8b7db3c9964f0573eba30ea977ff2/68747470733a2f2f636f6473706565642e696f2f6f675f696d6167652e706e67&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://codspeed.io\" title=\"CodSpeed: Optimize Performance, Eliminate Regressions\">CodSpeed: Optimize Performance, Eliminate Regressions</a></div><div class=\"message_embed_description\">CodSpeed integrates into dev and CI workflows to measure performance, detect regressions, and enable actionable optimizations.</div></div></div>",
        "id": 512690231,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744838615
    },
    {
        "content": "<p>if there's a GH related issue you'd like to pursue, I can faciliate that, as in <em>actually get that done</em> and not some \"other version of facilitate\".</p>",
        "id": 512820805,
        "sender_full_name": "Ralph",
        "timestamp": 1744894735
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"268586\">@Ralph</span> , at least for the moment I don't think there's an issue with GH per se, but I'll keep that in mind or come up with a set of questions to pass on once I have results from a more complete evaluation.</p>",
        "id": 512903378,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744919320
    },
    {
        "content": "<p>Could I get a squash-and-merge on <a href=\"https://github.com/bytecodealliance/sightglass/pull/284\">https://github.com/bytecodealliance/sightglass/pull/284</a>.  I've got another change (unrelated to nightly) that I want to queue up for review adding the ability to request multiple measures on a benchmark run.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/sightglass/pull/284\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/8ab45ad4ec27a83dcde06e0d04f1abbf09f3a765/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f623838383730333362383037353561336166333939303365333731653131313864636139383564653061623663643534663234356562373436633664303265382f62797465636f6465616c6c69616e63652f7369676874676c6173732f70756c6c2f323834&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/sightglass/pull/284\" title=\"Grab Bag of CI Enhancements by posborne · Pull Request #284 · bytecodealliance/sightglass\">Grab Bag of CI Enhancements by posborne · Pull Request #284 · bytecodealliance/sightglass</a></div><div class=\"message_embed_description\">This first set of changes includes the following:\n\nRemoves outdated github actions recipes that are no longer functioning.\nUpdates the benchmark flow to complete much more quickly by:\n\nDon&#39;t tr...</div></div></div>",
        "id": 512903606,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1744919406
    },
    {
        "content": "<p>Here's some boxplots showing the variability between benchmarks on a single host (in this case an arm mac) with 12 iterations / pass and 26 benchmark passes.  Some benchmarks appear to vary wildly (e.g. libsodium-box_easy2) in ways that make it unsuitable for comparison usage.  Others are short-lived and probably only useful with changes to do more iterations for certain tests (possibly removing instrumentation overhead in some way).</p>\n<p><a href=\"/user_uploads/15107/358JRxySJdMrxbBeSJ4a6s2T/sightglass-benchmark-timings.html\">sightglass-benchmark-timings.html</a></p>",
        "id": 513675703,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1745345910
    },
    {
        "content": "<p>This is really interesting; some of these look somewhat bimodal, e.g. the first one (blake3-scalar) has a bunch of runs around 130-140k cycles but regular outliers. I wonder if that could be due to heterogeneous SoC architectures, i.e., performance+efficiency cores in modern Macs?</p>",
        "id": 513680204,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1745347283
    },
    {
        "content": "<p>I wonder if it might be worth measuring variability with a benchmark that should be absolutely deterministic, too, to isolate system noise from any Sightglass or Wasmtime effects: e.g., a for-loop over 100M or 1B iterations or whatever, with <code>asm volatile(\"\");</code> in the middle to avoid optimizing away; if that has more than say 0.1% variability in runtime then the system is noisy</p>",
        "id": 513680492,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1745347392
    },
    {
        "content": "<p>(a for-loop in a native program, to be clear)</p>",
        "id": 513680538,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1745347409
    },
    {
        "content": "<p>Hmm, interesting theory on ecore/pcore scheduling.  I'll try to get some results on an x86 linux machine with core pinning to see how different things work.  On this same machine, I'm collecting results to compare for a somewhat known-overhead case of enabling epoch interrupts.</p>",
        "id": 513681598,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1745347770
    },
    {
        "content": "<p>The noop testing idea could be of use; I'm not sure if it is significantly better than just comparing standard deviation between historical runs to detect the same thing (though, of course, assuming execution variability wasn't somehow introduced by engine behavior, which seems fairly unlikely).</p>\n<p>In some environments, we can get the kernel-level preemptions, but I'm guessing for something like github actions runners we could be bumping into hypervisor-level contention.</p>",
        "id": 513682973,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1745348221
    },
    {
        "content": "<p>I've also wondered whether we should be looking more at minimums rather than mean or median. I'm not familiar enough with benchmarking literature to be sure but I feel like I've seen arguments somewhere that it's a reasonable choice for the kind of noise that occurs in benchmarking: most everything that interferes with the thing you're trying to measure can only make it take longer, not make it faster. A while ago I tried to find research answering this question one way or the other without much luck, so I didn't bring it up, but I don't know, maybe it would help here.</p>",
        "id": 513716874,
        "sender_full_name": "Jamey Sharp",
        "timestamp": 1745351734
    },
    {
        "content": "<p>Oh, interesting, I really like that; and it makes a lot of intuitive sense at least</p>",
        "id": 513717190,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1745351857
    },
    {
        "content": "<p>as long as there are reliably some runs that hit zero of the bottlenecks, I suppose -- a very noisy machine could have interruptions to every single benchmark run, but more runs reduces the probability of that</p>",
        "id": 513717368,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1745351924
    },
    {
        "content": "<p>I suspect there are some cases where the minimum isn't representative for specific optimizations where we might be more interested in reducing tail performance impacts but it doesn't feel like (accounting for frequency scaling as cycle counts <em>should</em>) there are too many things that could result in unrepresentative outliers on the minimum side of things.</p>",
        "id": 513718120,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1745352236
    },
    {
        "content": "<p>I've used the <code>noop.wasm</code> benchmark in the suite for this kind of baselining in the past.</p>",
        "id": 513719161,
        "sender_full_name": "Andrew Brown",
        "timestamp": 1745352627
    },
    {
        "content": "<p>A more principled approach might be fitting to an ex-gaussian distribution instead of a pure gaussian (<a href=\"https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\">https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution</a>), which accounts for skew. (I'd encountered that distribution when previously working in neuropsych but never understood before why it was useful…)</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/b7beb910643783745c1cc7b326efe5577f8290eb/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f312f31632f454d475f446973747269627574696f6e5f5044462e706e67&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://en.wikipedia.org/wiki/Exponentially_modified_Gaussian_distribution\" title=\"Exponentially modified Gaussian distribution - Wikipedia\">Exponentially modified Gaussian distribution - Wikipedia</a></div><div class=\"message_embed_description\">\n\n\n\nΦ\n(\nx\n,\nμ\n,\nσ\n)\n−\n\n\n1\n2\n\n\nexp\n⁡\n\n[\n\n\n\nλ\n2\n\n\n(\n2\nμ\n+\nλ\n\nσ\n\n2\n\n\n−\n2\nx\n)\n\n]\n\nerfc\n⁡\n\n(\n\n\n\nμ\n+\nλ\n\nσ\n\n2\n\n\n−\nx\n\n\n\n\n2\n\n\nσ\n\n\n\n)\n\n\n\n{\\displaystyle \\Phi (x,\\mu ,\\sigma )-{\\frac {1}{2}}\\exp \\left[{\\frac {\\lambda }{2}}(2\\mu +\\lambda \\sigma ^{2}-2x)\\right]\\operatorname {erfc} \\left({\\frac {\\mu +\\lambda \\sigma ^{2}-x}{{\\sqrt {2}}\\sigma }}\\right)}\n\n\nwhere\n</div></div></div>",
        "id": 513719170,
        "sender_full_name": "Jamey Sharp",
        "timestamp": 1745352631
    },
    {
        "content": "<p>Here's a run with fewer passes but doing a comparison of baseline (epoch interrupts disabled) with epoch interrupts enabled.  On some benchmarks, the overhead is clear and others it is a wash (not totally unexpected depending on the workload).</p>\n<p>For this smaller test run (again, on a laptop I was multitasking on) we do see cases where the epoch interrupt scores have minimum scores that are lower which probably doesn't represent reality (though probably close enough that we would say that these represent no difference).</p>\n<p><a href=\"/user_uploads/15107/0M9RFl-e2SDYZxdv_cJVCPSM/sightglass-benchmark-timings-compare.html\">sightglass-benchmark-timings-compare.html</a></p>",
        "id": 513721922,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1745353762
    },
    {
        "content": "<p><a href=\"https://github.com/bytecodealliance/sightglass/pull/286\">https://github.com/bytecodealliance/sightglass/pull/286</a> adds a tool for visualizing and comparing results; I'm using p25 quantiles as the value to compare but if we have consensus, it should be pretty easy to use something else.</p>\n<p>Of note, several of the benchmarks in the \"all\" suite today  have a huge standard deviation.  In the output, I'm using CV (coefficient of variation) as a way to showcase results that likely shouldn't be trusted as well for marking \"significant\" speedup/slowdowns.  The PR includes some data on tests I did against work that <span class=\"user-mention\" data-user-id=\"254083\">@Dan Gohman</span> did to pick up where <span class=\"user-mention\" data-user-id=\"504918\">@Jamey Sharp</span> left off on stack probe changes.</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/bytecodealliance/sightglass/pull/286\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/d4ccb4d5a55b4764af8a5f8fc9facc3518fd0eca/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f653463383738626165616565616630613839643762616662356366323461366132376335613462363161636564376361363238653836346563323239663565392f62797465636f6465616c6c69616e63652f7369676874676c6173732f70756c6c2f323836&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/bytecodealliance/sightglass/pull/286\" title=\"Add viz.py visualization tool by posborne · Pull Request #286 · bytecodealliance/sightglass\">Add viz.py visualization tool by posborne · Pull Request #286 · bytecodealliance/sightglass</a></div><div class=\"message_embed_description\">This tool is extracted from some early work I did in a jupyter notebook to anlayze and compare results relative to a baseline benchmark run.  The generated artificat is a single html file with both...</div></div></div>",
        "id": 515413336,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1746053525
    },
    {
        "content": "<p>I'll take a look tomorrow!</p>",
        "id": 515435051,
        "sender_full_name": "Andrew Brown",
        "timestamp": 1746067107
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"456180\">@Paul Osborne</span>, I was thinking about that PR after writing some thoughts there. I wanted to expand on the idea that we have (and need) a lot of flexibility when it comes to displaying the benchmark data: since it sounds like you've already been working in the Jupyter ecosystem, perhaps that could another option for the actual display of the results?</p>\n<p>What I mean is, instead of engineering a whole set of charts in <code>gh-pages</code>, what if we simply dump the benchmark results in a branch and use an external tool to examine them, like Jupyter? I see Google hosts Jupyter notebooks--can they be made publicly accessible? If so, our benchmarking UI story can be \"pull the results data from the branch\" into a Jupyter notebook. What I like about that approach is that (a) it's \"easy\" to create new visualizations by cloning a Jupyter notebook (b) we have all the existing Python/Jupyter ecosystem available to us, and (c) we avoid maintaining/hosting that infrastructure. I believe the <code>upload-elastic</code> command is a start in that direction in that it should be relatively easy to modify it to dump all the relevant result and fingerprint files into a directory instead of uploading them to a server (though that is an option, too).</p>\n<p>This is just brainstorming, so feel free to tell me why it's not a good idea. But what I'm getting at here is not so much a Jupyter-specific thing but rather the idea that we use external hosting infrastructure instead (Sheets? In-browser DuckDB? Etc.).</p>",
        "id": 515601997,
        "sender_full_name": "Andrew Brown",
        "timestamp": 1746144131
    },
    {
        "content": "<p>I think with the approach proposed in the PR, we'd get both.  The raw nightly benchmark data in csv/json would be available on the branch as well as the generated report.  So, anyone wanting to do analysis should be able to just pull the branch and run whatever tooling they want (jupyter/r/etc.).</p>\n<p>Pandas can even do <code>read_csv</code> from a URL which would be compatible with publishing to a branch as well.  We can document this workflow and provide some examples/notebooks if we like as well.</p>",
        "id": 515751037,
        "sender_full_name": "Paul Osborne",
        "timestamp": 1746202990
    }
]