[
    {
        "content": "<p>I compiled apache datafusion as a dependency of a wasm component just a few minutes ago, and that went fine, but then loading it in wasmtime takes ~22s! This is with all the things optimized like a --release build. I've attached an image of a profile. Is this expected? Are there ways I should be holding things differently that might help here?</p>\n<p><a href=\"/user_uploads/15107/cQ29beb1WlM59b_q2CCjPf39/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/15107/cQ29beb1WlM59b_q2CCjPf39/image.png\" title=\"image.png\"><img data-original-content-type=\"image/png\" data-original-dimensions=\"3440x503\" src=\"/user_uploads/thumbnail/15107/cQ29beb1WlM59b_q2CCjPf39/image.png/840x560.webp\"></a></div>",
        "id": 502133156,
        "sender_full_name": "Andrew Werner",
        "timestamp": 1740600000
    },
    {
        "content": "<p>To me that's roughly what I would expect in the breakdown of compiling a big program. Not to say there aren't low-hanging fruit to optimize though! Would you be able to share the wasm module in question?</p>\n<p>For your own development, you might want to try Winch as a compiler (e.g. <code>--compiler winch</code> on the CLI) which should have much speedier compile times.</p>\n<p>Also for improving this, often times historically I've seen that 99% of the compile time is one gargantuan function and by shrinking that function in the guest language itself you can often improve compile time</p>",
        "id": 502133731,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740600166
    },
    {
        "content": "<p>Thanks for the quick reply! Happy to share a wasm module, give me a few minutes. Will also try using twiggy to see what's dominating. Any other tools or advice to find that?</p>",
        "id": 502134259,
        "sender_full_name": "Andrew Werner",
        "timestamp": 1740600336
    },
    {
        "content": "<p>Here is the module: <br>\n<a href=\"/user_uploads/15107/ui3KKRfgY8-xWl89Ys38BwX6/wasm_playground_module.wasm\">wasm_playground_module.wasm</a></p>",
        "id": 502139413,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740602118
    },
    {
        "content": "<p>Winch does bring down loading the module down to 3.4s</p>",
        "id": 502141530,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740602877
    },
    {
        "content": "<p>The biggest function size is 0.14% per twiggy which seems not a lot; is there a better measure for finding function that is most complicated and time consuming for compiler?</p>",
        "id": 502142510,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740603255
    },
    {
        "content": "<p>actually, it is big, 164kb</p>",
        "id": 502142832,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740603368
    },
    {
        "content": "<p>One thing you can try is <code>WASMTIME_LOG=wasmtime_cranelift::compiler=debug wasmtime compile foo.wasm</code> and that'll print out things like:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"mi\">2025</span><span class=\"o\">-</span><span class=\"mi\">02</span><span class=\"o\">-</span><span class=\"mi\">26</span><span class=\"n\">T21</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mf\">38.580142</span><span class=\"n\">Z</span><span class=\"w\"> </span><span class=\"n\">DEBUG</span><span class=\"w\"> </span><span class=\"n\">wasmtime_cranelift</span><span class=\"p\">::</span><span class=\"n\">compiler</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">FuncIndex</span><span class=\"p\">(</span><span class=\"mi\">8519</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">translated</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"mf\">1.525569591</span><span class=\"n\">s</span>\n<span class=\"mi\">2025</span><span class=\"o\">-</span><span class=\"mi\">02</span><span class=\"o\">-</span><span class=\"mi\">26</span><span class=\"n\">T21</span><span class=\"p\">:</span><span class=\"mi\">04</span><span class=\"p\">:</span><span class=\"mf\">38.581608</span><span class=\"n\">Z</span><span class=\"w\"> </span><span class=\"n\">DEBUG</span><span class=\"w\"> </span><span class=\"n\">wasmtime_cranelift</span><span class=\"p\">::</span><span class=\"n\">compiler</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"nc\">FuncIndex</span><span class=\"p\">(</span><span class=\"mi\">8520</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"n\">translated</span><span class=\"w\"> </span><span class=\"k\">in</span><span class=\"w\"> </span><span class=\"mf\">1.040909</span><span class=\"n\">ms</span>\n<span class=\"o\">..</span><span class=\"p\">.</span>\n</code></pre></div>\n<p>which you can use to find functions that take a particularly long time to compile</p>",
        "id": 502144288,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740603926
    },
    {
        "content": "<p>long ones typically end up getting printed at the end</p>",
        "id": 502144303,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740603934
    },
    {
        "content": "<p>nothing stands out that much, slowest function is 90ms, there are 9k functions over 1ms</p>",
        "id": 502146716,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740604928
    },
    {
        "content": "<p>yeah I was gonna say I just finished downloading and nothing takes seconds on my machine, it's all ms-or-less</p>",
        "id": 502146893,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740605027
    },
    {
        "content": "<p>this is a pretty big module with 62k functions, and it's probably just \"we can probably apply elbow grease to make things faster\"</p>",
        "id": 502147021,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740605069
    },
    {
        "content": "<p>FWIW <span class=\"user-mention\" data-user-id=\"881259\">@Piotr Bejda</span> , the biggest parts of that compilation time, regalloc2 and the mid-end (egraph optimizer) are both things I wrote and spent months of time squeezing performance out of; there is probably not much low-hanging fruit left. Better results will come from taking different approaches: Winch as mentioned (baseline compiler, no regalloc or optimization at all), or perhaps a different register allocator (though the one that we're considering, regalloc3, goes in the other direction with slower compilation for a little more runtime perf)</p>",
        "id": 502147459,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1740605256
    },
    {
        "content": "<p>to set expectations, our compilation time is about 10x faster than LLVM, so from one perspective this is \"very fast\" already (for an optimizing compiler); it's just the nature of Wasm that one needs to have a compilation step when loading the module unfortunately, so if the baseline expectation is \"just loading a program\", it will feel pretty slow</p>",
        "id": 502147600,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1740605314
    },
    {
        "content": "<p><span class=\"user-mention\" data-user-id=\"881259\">@Piotr Bejda</span> <span class=\"user-mention\" data-user-id=\"881241\">@Andrew Werner</span> In case you aren't already aware: you can enable cwasm caching via <a href=\"https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.cache_config_load_default\">this config setting</a> which will avoid recompiling a wasm file if it hasn't changed since the last time it was run.</p>",
        "id": 502149079,
        "sender_full_name": "Joel Dice",
        "timestamp": 1740605892
    },
    {
        "content": "<p>makes sense, thank you all very much for sharing expertise! we might be able to use more bare parts of the datafusion, seems like higher abstractions are quite bloated compared to what we need</p>",
        "id": 502149485,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740606072
    },
    {
        "content": "<blockquote>\n<p>it's probably just \"we can probably apply elbow grease to make things faster\"</p>\n</blockquote>\n<p>Sorry I should clarify here -- I'm sure we have inefficiencies in schlepping around a lot of functions from cranelift to wasmtime and getting it all into a <code>*.cwasm</code> image. This is not a large part of your profile, though, and even if we were to optimize it may only help 1-2% (as a guess, usnure as to exact percentages)</p>\n<p>IIRC I think I saw a fuzz timeout of 1k empty functions awhile back so I do think there's stuff in that scalable area we have yet to improve. Improving regalloc/optimization though as Chris mentions is a much, much taller order</p>",
        "id": 502149575,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740606116
    },
    {
        "content": "<p>In terms of compile time another thing to mention is the incremental compilation support we have. It's not integrated into the CLI at this time but if you're using a rust embedding you might find it useful as it can improve compile times when only small portions of the module have changed</p>",
        "id": 502149701,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1740606156
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"509936\">Joel Dice</span> <a href=\"#narrow/channel/217126-wasmtime/topic/Slow.20module.20loading.20for.2040MiB.20program/near/502149079\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"881259\">Piotr Bejda</span> <span class=\"user-mention silent\" data-user-id=\"881241\">Andrew Werner</span> In case you aren't already aware: you can enable cwasm caching via <a href=\"https://docs.rs/wasmtime/latest/wasmtime/struct.Config.html#method.cache_config_load_default\">this config setting</a> which will avoid recompiling a wasm file if it hasn't changed since the last time it was run.</p>\n</blockquote>\n<p>yeah, we don't cache the loaded module, we will, but there are some scenarios where we want a process to start executing wasm fast from the get-go too (where that process should not completely trust the provided wasm binary)</p>",
        "id": 502149899,
        "sender_full_name": "Piotr Bejda",
        "timestamp": 1740606247
    },
    {
        "content": "<p>Makes sense.  I think that scenario is the reason Winch exists.</p>",
        "id": 502149979,
        "sender_full_name": "Joel Dice",
        "timestamp": 1740606292
    }
]