[
    {
        "content": "<p>Mission Accepted. Sounds to me like my first call of duty.</p>",
        "id": 561267117,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764620679
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"253994\">Alex Crichton</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561224516\">schrieb</a>:</p>\n<blockquote>\n<p>You can read up on LLVM's security policy <a href=\"https://llvm.org/docs/Security.html#what-is-considered-a-security-issue\">here</a>, notably:</p>\n<blockquote>\n<p>For example, a maliciously crafted C, Rust or bitcode input file can cause arbitrary code to execute in LLVM</p>\n</blockquote>\n<p>And this is NOT considered a security issue in LLVM since LLVM explicitly does not provide this guarantee</p>\n</blockquote>\n<p>In consequence:<br>\nIf I understand correctly, users of Emscripten (emcc), which leverages LLVM, are tasked with identifying potential CVEs from miscompilations. Is it accurate, then, to say that the fundamental difference between LLVM and Cranelift, in terms of miscompilation-induced CVE responsibility, is that LLVM places this burden on its users, while Cranelift's maintainers are the ones obligated to do so?</p>",
        "id": 561579245,
        "sender_full_name": "Thomas Trenner",
        "timestamp": 1764747696
    },
    {
        "content": "<p>Bugs in LLVM don't result in sandbox escapes with Emscripten given that Emscripten asks LLVM to output wasm that will run in the sandbox and thus no matter what LLVM outputs for (in)correct wasm, a sandbox escape would be impossible if the wasm engine doesn't have bugs. However if you were to compile malicious wasm to machine code with LLVM inside the wasm engine, that could potentially allow a sandbox escape if LLVM miscompiles something. In other words the difference between using LLVM to produce wasm and to compile was to machine code is whether LLVM is part of the <a href=\"https://en.wikipedia.org/wiki/Trusted_computing_base\">TCB</a> of the wasm engine or not.</p>",
        "id": 561613544,
        "sender_full_name": "bjorn3",
        "timestamp": 1764758780
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"253994\">Alex Crichton</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561223955\">said</a>:</p>\n<blockquote>\n<p>An example of this is that a miscompile in LLVM is not a CVE.</p>\n</blockquote>\n<p>I think this is not precisely right -- the page you link states:</p>\n<blockquote>\n<p>However, a miscompilation where there are clear indications that it can result in the produced binary becoming significantly easier to exploit could be considered security sensitive, and should be reported to the security response group</p>\n</blockquote>\n<p>Granted, it's a bit wishy-washy in the sense that \"ease of exploit\" depends on a threat model which is out of scope.  But even if not deemed to be a CVE, AFAIU miscompilations are always taken seriously -- like we're not arguing that if presented with a miscompilation that LLVM maintainers would say \"not my bug\", right?</p>\n<p>The argument here is not \"LLVM is more free to miscompile than wasmtime\" -- both are responsible for their output -- but rather \"an untrusted Wasm input lowered to LLVM IR may cause the LLVM middle-end to DOS / blow the stack / cause a UAF\" -- right?</p>",
        "id": 561619707,
        "sender_full_name": "Andy Wingo",
        "timestamp": 1764760011
    },
    {
        "content": "<p>Historically LLVM has considered miscompilations to not be CVEs. Only recently they started having a security policy. And there have been multiple miscompilations reachable in safe rust (and probably also from wasm) that have caused memory safety issues (and thus for wasm almost certainly sandbox escapes) And there are multiple miscompilations that have been open for years like <a href=\"https://github.com/llvm/llvm-project/issues/89885\">https://github.com/llvm/llvm-project/issues/89885</a> (when sse is disabled on x86, llvm const eval and runtime behavior mismatch for floats causing bound checks to be incorrectly optimized away as unreachable), <a href=\"https://github.com/rust-lang/rust/issues/70143\">https://github.com/rust-lang/rust/issues/70143</a> (stack probes are done after stack alignment, allowing stack smashing. not reachable for wasm), <a href=\"https://github.com/rust-lang/rust/issues/83060\">https://github.com/rust-lang/rust/issues/83060</a> (stack frames &gt;2GB getting miscompiled. not reachable for wasm)</p>\n<div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/llvm/llvm-project/issues/89885\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/5d1225fad8f53003756a2b2f3e20140a25a2e77c/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f363738613963623963353266336532656634323634303233366230323635656533336362393162633834396166393739363839626638373665356465356562362f6c6c766d2f6c6c766d2d70726f6a6563742f6973737565732f3839383835&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/llvm/llvm-project/issues/89885\" title=\"Difference between compile-time and runtime float precision on 32-bit x86 without SSE can cause miscompilation leading to segfault · Issue #89885 · llvm/llvm-project\">Difference between compile-time and runtime float precision on 32-bit x86 without SSE can cause miscompilation leading to segfault · Issue #89885 · llvm/llvm-project</a></div><div class=\"message_embed_description\">The following program (based on the Rust version from here, which is based on @comex's example from a different issue; they gave an explanation of how they found it here) #include &lt;stdio.h&gt; #includ...</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/rust-lang/rust/issues/70143\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/d798e6fdba2e814fda6135f7f21e70e805c78838/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f316334396631636566343834313139393339396135393133636533386162353439393465306432376466633335323030323135613162613361653938326335612f727573742d6c616e672f727573742f6973737565732f3730313433&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/rust-lang/rust/issues/70143\" title=\"Locals aligned to greater than page size can cause unsound behavior · Issue #70143 · rust-lang/rust\">Locals aligned to greater than page size can cause unsound behavior · Issue #70143 · rust-lang/rust</a></div><div class=\"message_embed_description\">Forked from #70022 Minimal example #[repr(align(0x10000))] struct Aligned(u8); fn main() { let x = Aligned(0); println!(\"{:#x}\", &amp;x as *const _ as usize); } Aligning the stack is done after the sta...</div></div></div><div class=\"message_embed\"><a class=\"message_embed_image\" href=\"https://github.com/rust-lang/rust/issues/83060\" style=\"background-image: url(&quot;https://uploads.zulipusercontent.net/6e0750c9d120e405199c83e4f98209515e7af4e0/68747470733a2f2f6f70656e67726170682e6769746875626173736574732e636f6d2f346265346635326533663163326362393435333235656239383837633731656638353530643462633563353034346665373339613533346533343033336463662f727573742d6c616e672f727573742f6973737565732f3833303630&quot;)\"></a><div class=\"data-container\"><div class=\"message_embed_title\"><a href=\"https://github.com/rust-lang/rust/issues/83060\" title=\"Regressions with large (2-4GB) stack arrays on large stacks · Issue #83060 · rust-lang/rust\">Regressions with large (2-4GB) stack arrays on large stacks · Issue #83060 · rust-lang/rust</a></div><div class=\"message_embed_description\">In the course of responding to someone's question, I was experimenting with large (1-4 GB) arrays on the stack, using threads with large stacks created using std::thread::Builder::stack_size. Code ...</div></div></div>",
        "id": 561627437,
        "sender_full_name": "bjorn3",
        "timestamp": 1764761717
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"569438\">Thomas Trenner</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561579245\">said</a>:</p>\n<blockquote>\n<p>If I understand correctly, users of Emscripten (emcc), which leverages LLVM, are tasked with identifying potential CVEs from miscompilations</p>\n</blockquote>\n<p>I wouldn't personally phrase it as this, as generalizing slightly you're conclusing that all users of LLVM (e.g. C, C++, Rust, etc, developers too) all have to worry about CVEs and practically that's not the status quo. The general reason for that is that when you're compiling someting you're effectively never compiling possibly-malicios code. You're compiling your project + dependencies so any problems along the way are ICEs or \"just bugs\".</p>",
        "id": 561685377,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1764776008
    },
    {
        "content": "<p>My comments about LLVM are about using it as a backend for compilation in a WebAssembly runtime. In such a scenario you are indeed compiling possibly-malicious code which is where this all comes up and is critically different from \"I'm a normal user of a toolchain\"</p>",
        "id": 561685584,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1764776050
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"396878\">Andy Wingo</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561619707\">said</a>:</p>\n<blockquote>\n<p>The argument here is not \"LLVM is more free to miscompile than wasmtime\" -- both are responsible for their output -- but rather \"an untrusted Wasm input lowered to LLVM IR may cause the LLVM middle-end to DOS / blow the stack / cause a UAF\" -- right?</p>\n</blockquote>\n<p>I wouldn't phrase it myself like this personally. I'd echo bjorn3 as well and summarize it as:</p>\n<ul>\n<li>In Wasmtime we evaluate every miscompile we find as to whether it's a CVE</li>\n<li>In LLVM I don't believe that every miscompile is evaluated as to whether it could break someone's sandbox</li>\n<li>Both projects, however, fix miscompiles ASAP as far as I know</li>\n</ul>\n<p>At least in the realm of miscompiles the main difference to me is how they're reviewed. I was also surprised to see that LLVM might consider some miscompiles as CVEs, that was definitely new to me. I would be pretty surprised though if every single bugfix going into LLVM, for every backend, was reviewed by some team somewhere as to whether it broke their sandbox and should be a CVE. Even if this did happen it would be problematic because that would mean that the bugfix would be public before it was determined to be a CVE, however.</p>\n<p>This also doesn't touch on the aspect of compiling itself. We issue CVEs for arbitrary code execution at compile time, and LLVM does not. I believe that's pretty fundamental to LLVM's and Cranelift's design and can't be changed with some simple review.</p>",
        "id": 561686931,
        "sender_full_name": "Alex Crichton",
        "timestamp": 1764776335
    },
    {
        "content": "<p>Yeah, personally I would also focus more on the security of the compiler invocation itself (broadly construed, including DoS) -- LLVM \"tries much harder\" and as a consequence, is easier than Cranelift to drive into pathological compile times or resource usage with malicious input. Cranelift was born in the context of browser engines where loading a new web page should never cause CPU to spike uncontrollably because one is compiling the (untrusted) code on page</p>",
        "id": 561696441,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1764778572
    },
    {
        "content": "<p>As an example, we're really careful about user input-controlled recursion (ie we avoid it) and in general are more suspicious of superlinear algorithms in Cranelift than LLVM is</p>",
        "id": 561696539,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1764778599
    },
    {
        "content": "<p>That's not to say we're a strict provable linear-time compiler (that's Winch, if you need it) but we try to have a much less spiky compile-time profile, and spikes are viewed as problems to be fixed</p>",
        "id": 561696674,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1764778632
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1001975\">Aarav Dayal</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561224989\">said</a>:</p>\n<blockquote>\n<p>wasmtime seems to be the best bet because of the strong backing by huge corporations.</p>\n</blockquote>\n<p>as compared to the strong backing for LLVM?</p>",
        "id": 562194423,
        "sender_full_name": "Chris Woods",
        "timestamp": 1764975010
    },
    {
        "content": "<p>Aarav is referring to Wasm engines specifically, I think, not compiler backends (though undoubtedly there are multiple Wasm engines with corporate backing as well)</p>",
        "id": 562195088,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1764975511
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"1001975\">Aarav Dayal</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561224793\">said</a>:</p>\n<blockquote>\n<p>Are there better code generators which give the best runtime performance as in my usecase startup times are a lesser priority over runtime performance.</p>\n</blockquote>\n<p>Ok. This thread took a hard left turn at security and kept going.</p>\n<p>Bringing it back to performance for a moment, you could compile your Wasm to a runtime and platform specific binary. This would be independent of the execution of the code.  So in essence:<br>\nOffline / Prep steps</p>\n<ol>\n<li>Receive Wasm</li>\n<li>Compile Wasm -&gt; AoT format (you can use LLVM here, in a container / vm etc)</li>\n</ol>\n<p>Then... when you need to execute</p>\n<ol start=\"3\">\n<li>Deploy AoT format to your blockchain VM.</li>\n</ol>\n<p>The benefits of this approach are:</p>\n<ol>\n<li>Smaller VM -&gt; you only need the infrastructure to support the AoT based runtime (for WAMR, as an example, it's about 30-50kb)</li>\n<li>Prep work is all done off line, as you allow your user to upload their code</li>\n<li>The Wasm compiler (back to security) would execute in a VM or other framework, isolating it from your infrastructure, so the compiler can spike / die / go on a new years party and drink too much wine, etc... and you can catch it and kill it, rejecting user code.</li>\n<li>When the code does execute in the VM - it's fast, very fast.</li>\n</ol>\n<p>I imagine that this is probably what most of the large scale cloud / server hosts of Wasm do today anyway.</p>\n<p>As Ralph said, everyone has a view on what is important. Think about your architecture, and what elements make the most sense for you to measure for performance.</p>\n<p>Best of luck with your project!</p>",
        "id": 562195269,
        "sender_full_name": "Chris Woods",
        "timestamp": 1764975634
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"254389\">Chris Fallin</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/562195088\">said</a>:</p>\n<blockquote>\n<p>Aarav is referring to Wasm engines specifically, I think, not compiler backends (though undoubtedly there are multiple Wasm engines with corporate backing as well)</p>\n</blockquote>\n<p><a href=\"/user_uploads/15107/rrYs7Gtmifj4cwqIyMc6kEqU/giphy.gif\">giphy.gif</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/15107/rrYs7Gtmifj4cwqIyMc6kEqU/giphy.gif\" title=\"giphy.gif\"><img data-animated=\"true\" data-original-content-type=\"image/gif\" data-original-dimensions=\"500x500\" src=\"/user_uploads/thumbnail/15107/rrYs7Gtmifj4cwqIyMc6kEqU/giphy.gif/840x560-anim.webp\"></a></div>",
        "id": 562195316,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764975668
    },
    {
        "content": "<p>Okay I will read all of this and then reply</p>",
        "id": 562195338,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764975688
    },
    {
        "content": "<p>Best of luck! :)</p>",
        "id": 562195353,
        "sender_full_name": "Chris Woods",
        "timestamp": 1764975701
    },
    {
        "content": "<p>Almost done. Just reading your message now.</p>",
        "id": 562195521,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764975846
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"435699\">Chris Woods</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/562195269\">said</a>:</p>\n<blockquote>\n<p><span class=\"user-mention silent\" data-user-id=\"1001975\">Aarav Dayal</span> <a href=\"#narrow/channel/206238-general/topic/Benchmarking.20Runtimes/near/561224793\">said</a>:</p>\n<blockquote>\n<p>Are there better code generators which give the best runtime performance as in my usecase startup times are a lesser priority over runtime performance.</p>\n</blockquote>\n<p>Ok. This thread took a hard left turn at security and kept going.</p>\n<p>Bringing it back to performance for a moment, you could compile your Wasm to a runtime and platform specific binary. This would be independent of the execution of the code.  So in essence:<br>\nOffline / Prep steps</p>\n<ol>\n<li>Receive Wasm</li>\n<li>Compile Wasm -&gt; AoT format (you can use LLVM here, in a container / vm etc)</li>\n</ol>\n<p>Then... when you need to execute</p>\n<ol start=\"3\">\n<li>Deploy AoT format to your blockchain VM.</li>\n</ol>\n<p>The benefits of this approach are:</p>\n<ol>\n<li>Smaller VM -&gt; you only need the infrastructure to support the AoT based runtime (for WAMR, as an example, it's about 30-50kb)</li>\n<li>Prep work is all done off line, as you allow your user to upload their code</li>\n<li>The Wasm compiler (back to security) would execute in a VM or other framework, isolating it from your infrastructure, so the compiler can spike / die / go on a new years party and drink too much wine, etc... and you can catch it and kill it, rejecting user code.</li>\n<li>When the code does execute in the VM - it's fast, very fast.</li>\n</ol>\n<p>I imagine that this is probably what most of the large scale cloud / server hosts of Wasm do today anyway.</p>\n<p>As Ralph said, everyone has a view on what is important. Think about your architecture, and what elements make the most sense for you to measure for performance.</p>\n<p>Best of luck with your project!</p>\n</blockquote>\n<p>There are atleast a few obvious issues with this approach:</p>\n<ol>\n<li>\n<p>Since each node could be on a different architecture, you can't really just compile a single AoT binary.</p>\n</li>\n<li>\n<p>Running a pre-compiled binary is literally an RCE and I am not sure if there is a good platform-independent way of implement fuel metering etc. especially if the program invokes syscalls, has static links, might even register signal handlers etc. This is literally a security vulnerability.</p>\n</li>\n<li>\n<p>If you instead post the optimized LLVM IR, then we are back to having to use LLVM on each machine which can cause resource hogging or even cause LLVM to crash or result into an RCE.</p>\n</li>\n</ol>\n<p>None the less, thank you for suggesting the sandboxing, I might take a look into this as in theory I can try to get it to work especially if I \"really\" want to squeeze out as much performance as I can.</p>\n<p>Here are a few questions I have:</p>\n<ol>\n<li>WAMR still runs wasm binaries and I am not sure whether it is still safe to use especially if it uses LLVM for AoT compilation as previously discussed and clarified in this thread. (unless there is some level of sandboxing which if so please kindly let me know.)</li>\n</ol>",
        "id": 562196197,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764976313
    },
    {
        "content": "<p>Also Chris thank you for the well wishes.</p>",
        "id": 562196230,
        "sender_full_name": "Aarav Dayal",
        "timestamp": 1764976328
    },
    {
        "content": "<p>FWIW, several Wasm engines do implement an AOT workflow -- Wasmtime does, and that's how it's been used in several places in FaaS contexts. The idea is that you have a control plane that runs the compiler and a VM that executes the artifact that results; the key bit is that <em>you</em> control the control-plane, and take measures (e.g. code-signing at least) to protect the integrity of the artifact that comes out of the AOT compiler half of the Wasm engine. You do also need the ability for your control plane to build the artifact for each architecture and configuration you run (Wasmtime embeds the config used for compilation in the precompiled artifact and checks that it matches at load-time). That said, everyone makes different tradeoffs, and I <em>also</em> know of a few places that use Wasmtime and load the Wasm / compile locally, because they don't want to deal with all that</p>",
        "id": 562199330,
        "sender_full_name": "Chris Fallin",
        "timestamp": 1764979013
    }
]