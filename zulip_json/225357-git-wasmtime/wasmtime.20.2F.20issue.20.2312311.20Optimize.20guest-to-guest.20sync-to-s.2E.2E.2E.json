[
    {
        "content": "<p>alexcrichton opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311\">issue #12311</a>:</p>\n<blockquote>\n<p>This is a meta/tracking issue about remaining work necessary to optimize the guest-to-guest sync-to-sync adapter generated by Wasmtime when component-model-async is enabled. Some more historical discussion of this happened at <a href=\"#narrow/channel/217126-wasmtime/topic/Wasmtime.20sync.3C-.3Esync.20adapter.20optimizability/near/563170820\">#wasmtime &gt; Wasmtime sync&lt;-&gt;sync adapter optimizability @ ðŸ’¬</a> as well, and I'll try to keep this up-to-date.</p>\n<h3>What is the problem</h3>\n<p>Wasmtime will compile an \"adapter\" with the FACT compiler when one guest component calls another. With the advent of component-model-async this adapter has a large number of permutations, for example the caller could be sync/async lowered, the callee could be sync/async lifted, and the function type itself could be sync or async. This specific issue is about the single case of a sync lowered caller, sync lifted callee, and sync function type. This doesn't mean the other permutations should be ignored, but that's the most interesting case for now.</p>\n<p>Additionally with the advent of component-model-async it's required, spec-wise, to manage async-task-related-infrastructure when crossing component boundaries. Task infrastructure comes into play in a number of scenarios, such as:</p>\n<ul>\n<li>When a task calls an imported function, that creates a new task. This new task has the current task as a parent task.</li>\n<li>Intrinsics such as <code>backpressure.{inc,dec}</code> modify the backpressure counter in the current task.</li>\n<li>When a task exits/returns all of its pending subtasks are \"reparented\" to the task's own parent.</li>\n</ul>\n<p>Effectively, there's substantial infrastructure pieces that may be used across component boundaries, and thus Wasmtime needs to handle this. This leads us to the problem: with component-model-async disabled this task management is all ignored as it's not applicable, but with component-model-async enabled this task management is enabled. This means that the sync&lt;-&gt;sync adapter will call a host function to manage task infrastructure pieces.</p>\n<p>This cost of this hostcall is relative to the situation of the adaptation being performed, but the goal of sync&lt;-&gt;sync adapter is to, ideally, compile to a grand total of 0 instructions. Given that it's impossible to optimize away a call into the host, this issue is thus about the problem of solving the task infrastructure management problem without actually making a host call. This should restore the prior-to-component-model-async behavior of a sync&lt;-&gt;sync adapter compiling to pure optimizable CLIF which mostly boils away.</p>\n<h3>History and Current Status</h3>\n<p>As of the time of this writing Wasmtime doesn't actually do any manipulation of task infrastructure on sync&lt;-&gt;sync adapters. This is a bug and results in issues such as <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12128\">https://github.com/bytecodealliance/wasmtime/issues/12128</a> (plus many undocumented others we have since realized). @dicej will soon have a PR to fix this situation where task infrastructure will be maintained across these boundaries.</p>\n<p>The plan is to have a PR which will enhance the sync&lt;-&gt;sync adapter with task infrastructure management, conditionally. The condition will be based on whether the <code>component-model-async</code> wasm feature is enabled in the <code>Config</code>. This is intended to be a stopgap because embedders should not need to disable features for performance.  For the time being though it'll retain the pre-p3 performance profile of sync adapters while retaining p3-relatevant spec compliance.</p>\n<h3>Future plans for optimization</h3>\n<p>Enabling Cranelift to compile these adapters to zero instructions is going to require special care and a number of refactorings of Wasmtime's task infrastructure in addition to new Cranelift optimizations. The general rough idea for the implementation is:</p>\n<ul>\n<li>A new <code>VMAsyncTask</code> type will be added. Fields this will contain are:<ul>\n<li>A \"kind\", more relevant in a moment</li>\n<li>Fields for <code>context.set {0,1}</code></li>\n<li>A parent pointer for the parent task. <code>Option&lt;NonNull&lt;VMAsyncTask&gt;&gt;</code></li>\n<li>Backpressure fields (if necessary still, we've talked about removing backpressure)</li>\n<li>A flag of whether this task can block or not.</li>\n</ul>\n</li>\n<li>The Rust-based \"full\" async task will contain this field as well as any other tables and such necessary. This will be similar to <code>VMContext</code> vs <code>vm::Instance</code>, for example.</li>\n<li>The current task will be stored in <code>VMContext</code> or <code>VMComponentContext</code> (maybe both? unsure?)</li>\n<li>Sync&lt;-&gt;sync adapters will allocate, on the stack, a <code>VMAsyncTask</code> with just these fields. This will be initialized with the current task and then the current task will be set to this.</li>\n<li>Manipulations of the current task will go directly through <code>VMAsyncTask</code> if applicable, e.g. <code>context.{g,s}et {0,1}</code></li>\n<li>Manipulations of the current task that require Rust data structures, for example adding a subtask, will \"promote\" the task from the stack to the Rust heap. This will go back through the entire chain of tasks and promote them all to the heap most likely too.</li>\n<li>Returning from a sync&lt;-&gt;sync adapter will restore the current task to its previous value.</li>\n</ul>\n<p>Effectively, at a high level, sync&lt;-&gt;sync adapters will allocate a task on the stack that, if necessary, will get promoted to the Rust heap to perform more expensive maniuplations on. In essence Rust-level tasks are lazily created only as necessary for \"more complicated\" things, like spawning subtasks, while low-level actions like <code>context.get</code> will remain efficient.</p>\n<p>The resulting CLIF for a sync&lt;-&gt;sync adapter will pseudo-code look like:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">void</span><span class=\"w\"> </span><span class=\"n\">adapter</span><span class=\"p\">(</span><span class=\"n\">vmctx</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"n\">vmctx</span><span class=\"p\">)</span><span class=\"w\"> </span><span class=\"p\">{</span>\n<span class=\"w\">    </span><span class=\"n\">vmtask</span><span class=\"w\"> </span><span class=\"o\">*</span><span class=\"n\">prev_head</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">vmctx</span><span class=\"p\">-&gt;</span><span class=\"nc\">current_task</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"n\">vmtask</span><span class=\"w\"> </span><span class=\"n\">stack_node</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"n\">stack_node</span><span class=\"p\">-&gt;</span><span class=\"nc\">kind</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">VMTASK_STACK</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"c1\">// ...</span>\n<span class=\"w\">    </span><span class=\"n\">stack_node</span><span class=\"p\">-&gt;</span><span class=\"nc\">prev</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">prev_head</span><span class=\"p\">;</span>\n<span class=\"w\">    </span><span class=\"n\">vmctx</span><span class=\"p\">-&gt;</span><span class=\"nc\">current_task</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"o\">&amp;</span><span class=\"n\">stack_node</span><span class=\"p\">;</span>\n\n<span class=\"w\">    </span><span class=\"n\">the_callee_component</span><span class=\"p\">(</span><span class=\"n\">vmctx</span><span class=\"p\">);</span>\n\n<span class=\"w\">    </span><span class=\"n\">vmctx</span><span class=\"p\">-&gt;</span><span class=\"nc\">current_task</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">prev_head</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div>\n<p>If <code>the_callee_component(vmctx)</code> is small enough the theory here is:</p>\n<ul>\n<li>Cranelift will see that <code>vmctx-&gt;current_task</code> is loaded, stored to, then stored to with the previous value. If <code>the_callee_component(vmctx)</code> has no obviously aliasing regions, then it can eliminate both stores as dead.</li>\n<li>If <code>the_callee_component</code> doesn't actually do anything like call the host then Cranelift will see that all the stores to <code>stack_node</code> are unused, so they're all eliminated.</li>\n<li>If all the previous loads/stores were eliminated, then the load from <code>vmctx-&gt;current_task</code> is also dead, so that's also eliminated.</li>\n</ul>\n<p>I don't believe that Cranelift will perform all of these optimizations, but my understanding so far is that this is well within Cranelift's complexity budget and wheelhouse to implement optimizations like these.</p>\n<h3>Expected Timeline</h3>\n<p>The current plan is to ship the hostcall-to-manipulate-task-infrastructure with WASIp3 originally. Embeddings that need the highest performance on sync&lt;-&gt;sync adapters will disable the <code>component-model-async</code> runtime feature (and maybe compile time feature). After WASIp3 ships and we have enough time to come back to this and design this all \"for real\" we'll implement this. At that point it won't matter if engines turn the <code>component-model-async</code> feature on-or-off, it'll be the same.</p>\n<p>Another point to note here is that it's expected that in WASIp3 Wasmtime will need to pretty heavily optimize calls to <code>context.{get,set}</code>. This work, while not the same as optimizing get/set, is highly related and will likely be a prerequisite for this work. That's to say that this work isn't solely motivated by sync&lt;-&gt;sync adapters, but instead it's motivated by other routes too.</p>\n</blockquote>",
        "id": 567268085,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1767995565
    },
    {
        "content": "<p><a href=\"https://github.com/alexcrichton\">alexcrichton</a> added the wasm-proposal:component-model-async label to <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311\">Issue #12311</a>.</p>",
        "id": 567268088,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1767995565
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311#issuecomment-3730749491\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311\">issue #12311</a>:</p>\n<blockquote>\n<blockquote>\n<p>If the_callee_component doesn't actually do anything like call the host then Cranelift will see that all the stores to stack_node are unused, so they're all eliminated.</p>\n</blockquote>\n<p>Unless I'm misunderstanding the problem statement, I think this is outside the scope of ordinary dead-store elimination or the sort of thing Cranelift would tackle: it implies interprocedural program analysis, which is fundamentally hard.</p>\n<p>Said another way: you're pushing a local alloc onto a linked list, then calling some arbitrary code; absent some global analysis, we can't know that that code won't eventually reach some behavior that will require observing that list, right? And that global analysis would need to reason about the callgraph, which depends on a value-range analysis and points-to analysis, both of which are extremely expensive, imprecise (overly conservative / brittle, easy to collapse with the wrong operator), or both.</p>\n<p>Separately, we'd also need an escape analysis to not do that local alloc at all, right? That's a whole separate can of worms. Possible, but complex.</p>\n<p>Overall: I'd be somewhat concerned waiting for a \"sufficiently smart compiler\" to get good component-to-component call performance; while it is definitely within scope to build new optimizations, trying to derive-from-first-principles why the code we emitted is unnecessary is always less preferable than modifying the runtime (or spec?) so we don't need that code.</p>\n</blockquote>",
        "id": 567270467,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1767996861
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311#issuecomment-3730780665\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311\">issue #12311</a>:</p>\n<blockquote>\n<p>No no, I understand that interprocedural analysis is off the table. I can try to expand more on this in a Cranelift meeting if desired to double-check the optimizations are in-scope.</p>\n<p>What I want Cranelift to be able to optimize is something like:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">v0</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">stack_addr</span><span class=\"w\"> </span><span class=\"p\">;;</span><span class=\"w\"> </span><span class=\"n\">some</span><span class=\"w\"> </span><span class=\"n\">stack</span><span class=\"o\">-</span><span class=\"n\">based</span><span class=\"w\"> </span><span class=\"n\">node</span>\n<span class=\"n\">v1</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">vmctx</span><span class=\"o\">+</span><span class=\"mh\">0x100</span><span class=\"w\"> </span><span class=\"p\">;</span><span class=\"w\"> </span><span class=\"n\">load</span><span class=\"w\"> </span><span class=\"n\">prev</span>\n<span class=\"n\">store</span><span class=\"w\"> </span><span class=\"n\">vmctx</span><span class=\"o\">+</span><span class=\"mh\">0x100</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">v0</span>\n<span class=\"p\">;;</span><span class=\"w\"> </span><span class=\"n\">some</span><span class=\"w\"> </span><span class=\"n\">inlined</span><span class=\"w\"> </span><span class=\"n\">version</span><span class=\"w\"> </span><span class=\"n\">of</span><span class=\"w\"> </span><span class=\"err\">`</span><span class=\"n\">the_callee_component</span><span class=\"err\">`</span><span class=\"w\"> </span><span class=\"n\">that</span><span class=\"w\"> </span><span class=\"n\">clearly</span><span class=\"w\"> </span><span class=\"n\">doesn</span><span class=\"o\">'</span><span class=\"na\">t</span><span class=\"w\"> </span><span class=\"n\">store</span><span class=\"w\"> </span><span class=\"n\">to</span><span class=\"w\"> </span><span class=\"n\">vmctx</span><span class=\"w\"> </span><span class=\"n\">based</span><span class=\"w\"> </span><span class=\"n\">on</span><span class=\"w\"> </span><span class=\"n\">alias</span><span class=\"w\"> </span><span class=\"n\">analysis</span>\n<span class=\"n\">store</span><span class=\"w\"> </span><span class=\"n\">vmctx</span><span class=\"o\">+</span><span class=\"mh\">0x100</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"n\">v1</span>\n</code></pre></div>\n<p>Here the first store is dead since it's never read, so it's eliminated. It's also into the vmctx so it's trusted/notrap/etc. The second store is then the same as what was loaded, so there's no need to load-then-store, so it's eliminated. Then the load is eliminated because it's dead.</p>\n<blockquote>\n<p>I'd be somewhat concerned waiting for a \"sufficiently smart compiler\" to get good component-to-component call performance</p>\n</blockquote>\n<p>Oh don't worry, I've worked long enough with Rust and optimizations that a sufficiently-smart-compiler is \"this either works with simple-ish heuristics or not at all\". </p>\n<p>Basically I didn't explicitly say that <code>the_callee_component(vmctx)</code> was inlined, but for all optimizations above I meant \"this optimization is only applicable when the entire body is fully inlined\". The puropse is to ensure component functions using unsafe intrinsics, which are expected to be fully inlined, to boil away the surrounding infrastructure</p>\n</blockquote>",
        "id": 567271336,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1767997381
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311#issuecomment-3730789252\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/12311\">issue #12311</a>:</p>\n<blockquote>\n<p>Ah, I see -- yeah, if we're also assuming cross-component inlining then this is again intraprocedural, and at least tractable. Thanks for the clarification!</p>\n</blockquote>",
        "id": 567271634,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1767997588
    }
]