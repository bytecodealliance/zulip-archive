[
    {
        "content": "<p>cfallin opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 and followups, so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin.</li>\n<li>[ ] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments.</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 548331825,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762024295
    },
    {
        "content": "<p>cfallin assigned cfallin to <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>.</p>",
        "id": 548331987,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762024489
    },
    {
        "content": "<p><a href=\"https://github.com/cfallin\">cfallin</a> added the wasmtime:debugging label to <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">Issue #11964</a>.</p>",
        "id": 548331995,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762024495
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin.</li>\n<li>[ ] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments.</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 553434866,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762189435
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin.</li>\n<li>[ ] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (pending in #11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 553736071,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762299614
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin.</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (done in #11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 553747172,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762306705
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin.</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (done in #11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558329784,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763599060
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (done in #11895).</li>\n<li>[ ] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin. (pending in #12052)</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (done in #11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (I have a version of this in a private branch that I'll extract soon.)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558335113,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763603015
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558776990,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763777768
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[ ] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558784012,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763786031
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558914486,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763923829
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (pending in #12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself.</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 558914490,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763923831
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[ ] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 561320445,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1764648010
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[ ] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 561320451,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1764648016
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint.</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC.</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks.</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 561550107,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1764726346
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[ ] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 562271851,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765063955
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[ ] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 563556638,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765571589
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[ ] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[x] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 563556639,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765571590
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[x] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[x] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[ ] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 563556648,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765571593
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[x] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[x] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[x] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</li>\n<li>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 563556664,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765571599
    },
    {
        "content": "<p>cfallin edited <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[x] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[x] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[x] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li><del>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</del> (Not needed for gdbstub protocol)</li>\n<li><del>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</del> (Move to post-MVP)</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 564749333,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1766166487
    },
    {
        "content": "<p>cfallin closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>After offline discussion with @alexcrichton and @fitzgen, we've discussed some of the design choices that were brought up in discussion in #11826, #11921, #11930, and elsewhere, and settled on a reasonable path for \"simplest possible debug instrumentation that can work\". I wanted to document that here as a meta-issue with a checklist.</p>\n<h2>Background and Main Choice: Hardware vs. Software Debugger Entries</h2>\n<p>To start: a debugger of bytecode-compiled-to-native-code needs to be able to</p>\n<ul>\n<li>Inspect/recover bytecode-level program state from the native code, somehow, when observing paused stack frames;</li>\n<li>Receive control when a trap occurs that would kill the instance;</li>\n<li>Insert additional points (\"breakpoints\"/\"watchpoints\") where it can receive control and then eventually return it (\"resumable traps\" in a sense).</li>\n</ul>\n<p>We have the state inspection part covered in #11769 (built on #11768 and #11783, and with #11873 and #11899 as followups), and we also have a callback/hook framework to register a debugger and listen for events (#11895), so the remaining focus is how to build the \"control-flow interjection\" aspect.</p>\n<p>There is a fundamental choice in the design space: we could either make maximal use of hardware traps, and redirect them to the debugger; <em>including</em> creating <em>new</em> scenarios where a hardware trap occurs, for debugger purposes (e.g. patching in \"break\" instructions for breakpoints that raise <code>SIGILL</code>, and resuming by jumping past them). Or we could perform all checks for trapping conditions, breakpoint conditions, etc., in software, and do a \"normal\" libcall into the runtime.</p>\n<p>Some aspects of the tradeoff are:</p>\n<ul>\n<li>\n<p>Redirecting traps is very complex and subtle, and depends on the combination of the ISA and OS/platform. We already have some specific code at each intersection point of ISA and OS to handle signals, but most of this is factored (general code for \"Unix signals\", and just a few lines to get the right registers on x86-64 or aarch64 or ...). In contrast, in #11930 we see that we need a full assembly stub and strategy for each ISA+OS, and sometimes ISA variants (RISC-V with or without vectors, for example) too.</p>\n<p>This is entirely possible to build and maintain, but the complexity does imply ongoing maintenance cost, and also some additional risk as this is a core load-bearing part of the runtime (trap handling generally).</p>\n</li>\n<li>\n<p>Along with that, while in principle it's possible to turn traps into \"virtual libcalls\", the pointer provenance story is nontrivial: we need to be able to recover the current store from TLS and plumb that into the libcall, but all libcalls today take the current instance (<code>vmctx</code>) as their first arg instead; and also we already have pointers to a few pieces of the store, but not the whole thing, in our TLS structure; and also some points in the matrix above (macOS in particular) don't give access to TLS during the trap-redirection phase; and also all of this requires careful reasoning about ownership and held/cached state of the store in the Cranelift code too (see the last bullet-point in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11921#discussion_r2462133985\">this</a> comment for more), as this is adding an implicit call with mut store borrow on every trapping instruction.</p>\n</li>\n<li>\n<p>On the other hand, giving up any trap-based mechanism imposes more runtime cost:</p>\n<ul>\n<li>It means that we have to turn off signal-based traps (conveniently we already have and test this option!), which implies a 1.5-2x slowdown, mainly due to explicit bounds checking for memories.</li>\n<li>It means that we can't use a patchable single instruction (<code>ud2</code> on x86-64 / <code>brk</code> on aarch64 / ...) for breakpoints, adding some code size.</li>\n</ul>\n</li>\n</ul>\n<p>Despite the increased cost, the complexities of call injection on traps are significant and this has pushed us to limit scope and make the software-based approach work. That, in turn, has led to some brainstorming around bringing that cost down. To that end, the plan...</p>\n<h2>Plan: Software-based with Patchable Code</h2>\n<p>Our plan for a simple yet reasonably performant debugger that can handle traps and insert breakpoints is:</p>\n<ul>\n<li>[x] Augment traps raised by the <code>trap</code> libcall with a debugger callback point (#11895).</li>\n<li>[x] Turn off signal-based traps when debugging is enabled. That means that every trap becomes a libcall to the <code>trap</code> builtin (#12052).</li>\n<li>[x] Ensure that Pulley uses libcall traps too. Currently <a href=\"https://github.com/bytecodealliance/wasmtime/blob/d28086570271fef3af39b5c5f6891639a680460f/crates/cranelift/src/func_environ.rs#L4488-L4496\">here</a> Pulley is special-cased to continue to rely on trapping instructions because the \"traps\" are handled in the interpreter rather than with true OS signals. Modifying that conditional leads to a few cases of missing instruction lowerings that I'm working through, but otherwise this should \"just work\". At this point, the debugger can now catch all traps in native and Pulley environments (#11982).</li>\n<li>[x] Modify the runtime to allow private copies of code to exist for individual instances of a module. This will allow us to flip permissions to RW, patch code, and flip back to RX whenever we have control in the runtime without fear of race conditions, and without impact on other instances of the same module. (#12051) That lets us build a more efficient mechanism for...</li>\n<li>[x] ... breakpoints with <em>patchable calls</em>. The idea here is to implement<ul>\n<li>[x] A new ABI that has <em>no clobbers</em> (no caller-saves) and takes arg(s) only in registers; this lets callsites be single call instructions and guarantees no impact on regalloc aside from fixed-reg constraints. The idea is that we can have a function that we invoke on breakpoint that will have no impact on the common case where we do not call it. (#12061)</li>\n<li>[x] A new \"patchable call\" instruction in Cranelift that emits a normal call, restricted only to functions with the above new ABI (call it the \"patchable callee ABI\"?). This means we don't have to use the full callsite emission implementation and can emit a single call instruction with the right register constraints. The idea is that the MachBuffer will contain new metadata that indicates \"patchable callsites\" and specifies the bytes to patch in to enable or disable the call; we can do so by patching in appropriate NOP(s) or the call itself. (#12101)</li>\n<li>[x] A modification to the sequence-point emission that adds a patchable call at every Wasm sequence point so we can patch in a breakpoint. (#12133)</li>\n<li>[x] Then we can implement a debugger API to enable any given breakpoint by Wasm PC. (#12133)</li>\n</ul>\n</li>\n<li>[x] Then we can implement \"step\" using breakpoints. For simplicity, for \"step\", let's start by patching in all breakpoint calls in all modules in the store. My hypothesis here is that while the debugger API may be used in an automated way driven by higher-level algorithms (e.g. reversible execution), flipping back and forth between \"step\" mode and \"sparse breakpoint set\" mode likely only happens at human speed, so it's fine to potentially patch a few megabytes of calls in the worst case. If that ends up not the case, we can do function-at-a-time patching of all breakpoints by adding separate function entry/exit hooks. (#12133)</li>\n<li><del>[ ] Then we can implement \"next\" using breakpoints as well, plus function entry and exit hooks.</del> (Not needed for gdbstub protocol)</li>\n<li><del>[ ] Then we can implement memory watchpoints using a shadow memory as described in the RFC.</del> (Move to post-MVP)</li>\n</ul>\n<p>To highlight the new thinking/insights here: if we don't do call injection on traps, then the two lost capabilities are using hardware to catch normal Wasm traps, and using hardware to do very efficient \"patching in of breakpoints\". But patchable calls are nearly as good for the latter (exactly as good on aarch64, 2 bytes vs 5 bytes on x86-64; the ABI is key to ensuring only the call instruction itself is needed); so the \"only\" loss is that we need explicit bounds-checking, and we can probably live with that.</p>\n<p>I have WIP branches for the patchable ABI, for private code, and for Pulley to use libcall-based traps universally; I'll keep working through the checklist above as time allows.</p>\n</blockquote>",
        "id": 564752852,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1766167818
    },
    {
        "content": "<p>cfallin <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964#issuecomment-3676035765\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11964\">issue #11964</a>:</p>\n<blockquote>\n<p>I've pushed the watchpoints item (last bullet-point) off to #12188. With that, I believe all the other tasks here are complete and the API is more or less sufficient to build a basic gdbstub-like interface to a top half (which I now intend to do!).</p>\n</blockquote>",
        "id": 564752853,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1766167818
    }
]