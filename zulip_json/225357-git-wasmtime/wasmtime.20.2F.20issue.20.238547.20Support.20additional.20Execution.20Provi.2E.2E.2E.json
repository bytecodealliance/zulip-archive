[
    {
        "content": "<p>kaivol opened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<h4>Feature</h4>\n<p>Currently the ONNX backend in <code>wasmtime-wasi-nn</code> only uses the default CPU execution provider and ignores the <code>ExecutionTarget</code> requested by the WASM caller. <br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33\">https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33</a></p>\n<p>I would like to suggest adding support for additional execution providers (CUDA, TensorRT, ROCm, ...) to <code>wasmtime-wasi-nn</code>. </p>\n<h4>Benefit</h4>\n<p>Improved performance for WASM modules using the <code>wasi-nn</code> API. </p>\n<h4>Implementation</h4>\n<p><code>ort</code> already has support for many execution providers, so integrating these into <code>wasmtime-wasi-nn</code> should not be to much work.<br>\nI would be interested in looking into this, however, I only really have the means to test the DirectML and NVIDIA CUDA / TensorRT EPs. </p>\n<h4>Alternatives</h4>\n<p>Leave it to the users to add support for additional execution providers. </p>\n</blockquote>",
        "id": 436962710,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1714772328
    },
    {
        "content": "<p>abrown <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-2161258990\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>I was looking at old issues and ran across this one (sorry for such a late reply!): I completely agree with this idea. I am tempted to say \"go for it!\" but maybe there is some coordination needed. E.g., I think @jianjunz has started enabling some DirectML bits in #8756. And @devigned may have some opinions on the best way to do this. But from my perspective, this seems like a worthwhile avenue to pursue.</p>\n</blockquote>",
        "id": 444058339,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1718126349
    },
    {
        "content": "<p>devigned <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-2180629433\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>I think this is a great idea! One interesting part will be testing. We may need to spin up some hardware to make sure the functionality stays evergreen. </p>\n</blockquote>",
        "id": 445838309,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1718888568
    },
    {
        "content": "<p>zhen9910 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-3513706928\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>Is there any update for this open issue?  We also want to use GPU/NPU for ONNX backend for our projects. If not available yet, I may try to implement a solution and want to get some suggestions.</p>\n</blockquote>",
        "id": 554781845,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762805171
    },
    {
        "content": "<p>zhen9910 edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-3513706928\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>Is there any update for this open issue?  We also want to use GPU/NPU for ONNX backend for our projects. If not available yet, I may try to take a look and want to get some suggestions.</p>\n</blockquote>",
        "id": 554806747,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1762815498
    },
    {
        "content": "<p>zhen9910 <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-3549384162\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>I made some changes to support GPU for onnx backend, please review <a href=\"https://github.com/bytecodealliance/wasmtime/pull/12044\">https://github.com/bytecodealliance/wasmtime/pull/12044</a></p>\n<p>It updated the onnxruntime crate <code>ort</code> to 2.0.0-rc.10 with improved cuda support and added onnx-cuda in <code>wasmtime-wasi-nn</code> onnx backend. I verified it using an Azure-VM installed with Nvidia A10 GPU. </p>\n</blockquote>",
        "id": 558032424,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1763496419
    },
    {
        "content": "<p>abrown closed <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<h4>Feature</h4>\n<p>Currently the ONNX backend in <code>wasmtime-wasi-nn</code> only uses the default CPU execution provider and ignores the <code>ExecutionTarget</code> requested by the WASM caller. <br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33\">https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33</a></p>\n<p>I would like to suggest adding support for additional execution providers (CUDA, TensorRT, ROCm, ...) to <code>wasmtime-wasi-nn</code>. </p>\n<h4>Benefit</h4>\n<p>Improved performance for WASM modules using the <code>wasi-nn</code> API. </p>\n<h4>Implementation</h4>\n<p><code>ort</code> already has support for many execution providers, so integrating these into <code>wasmtime-wasi-nn</code> should not be to much work.<br>\nI would be interested in looking into this, however, I only really have the means to test the DirectML and NVIDIA CUDA / TensorRT EPs. </p>\n<h4>Alternatives</h4>\n<p>Leave it to the users to add support for additional execution providers. </p>\n</blockquote>",
        "id": 562815170,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765320073
    },
    {
        "content": "<p>abrown <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-3634585888\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p>I'm going to close this in favor of #12044.</p>\n</blockquote>",
        "id": 562815172,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765320073
    },
    {
        "content": "<p>abrown reopened <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<h4>Feature</h4>\n<p>Currently the ONNX backend in <code>wasmtime-wasi-nn</code> only uses the default CPU execution provider and ignores the <code>ExecutionTarget</code> requested by the WASM caller. <br>\n<a href=\"https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33\">https://github.com/bytecodealliance/wasmtime/blob/24c1388cd74ab321d60af147fc074d12166258fd/crates/wasi-nn/src/backend/onnxruntime.rs#L21-L33</a></p>\n<p>I would like to suggest adding support for additional execution providers (CUDA, TensorRT, ROCm, ...) to <code>wasmtime-wasi-nn</code>. </p>\n<h4>Benefit</h4>\n<p>Improved performance for WASM modules using the <code>wasi-nn</code> API. </p>\n<h4>Implementation</h4>\n<p><code>ort</code> already has support for many execution providers, so integrating these into <code>wasmtime-wasi-nn</code> should not be to much work.<br>\nI would be interested in looking into this, however, I only really have the means to test the DirectML and NVIDIA CUDA / TensorRT EPs. </p>\n<h4>Alternatives</h4>\n<p>Leave it to the users to add support for additional execution providers. </p>\n</blockquote>",
        "id": 562815216,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765320092
    },
    {
        "content": "<p>abrown edited a <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547#issuecomment-3634585888\">comment</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/issues/8547\">issue #8547</a>:</p>\n<blockquote>\n<p><del>I'm going to close this in favor of #12044.</del> Never mind, this is an issue resolved by #12044.</p>\n</blockquote>",
        "id": 562815281,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1765320122
    }
]