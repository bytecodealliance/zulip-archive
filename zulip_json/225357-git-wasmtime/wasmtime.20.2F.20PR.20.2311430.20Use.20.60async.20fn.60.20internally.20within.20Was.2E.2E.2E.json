[
    {
        "content": "<p>alexcrichton opened <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a> from <code>alexcrichton:internal-async-functions</code> to <code>bytecodealliance:main</code>:</p>\n<blockquote>\n<p>This commit is an initial step towards resolving <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11262\">https://github.com/bytecodealliance/wasmtime/issues/11262</a> by having async<br>\nfunctions internally Wasmtime actually be <code>async</code> instead of requiring<br>\nthe use of fibers. This is expected to have a number of benefits:</p>\n<ul>\n<li>\n<p>The Rust compiler can be used to verify a future is <code>Send</code> instead of<br>\n  \"please audit the whole codebase's stack-local variables\".</p>\n</li>\n<li>\n<p>Raw pointer workarounds during table/memory growth will no longer be<br>\n  required since the arguments can properly be a split borrow to data in<br>\n  the store (eventually leading to unblocking <a href=\"https://github.com/bytecodealliance/wasmtime/issues/11178\">https://github.com/bytecodealliance/wasmtime/issues/11178</a>).</p>\n</li>\n<li>\n<p>Less duplication inside of Wasmtime and clearer implementations<br>\n  internally. For example GC bits prior to this PR has duplicated<br>\n  sync/async entrypoints (sometimes a few layers deep) which eventually<br>\n  bottomed out in <code>*_maybe_async</code> bits which were <code>unsafe</code> and require<br>\n  fiber bits to be setup. All of that is now gone with the <code>async</code><br>\n  functions being the \"source of truth\" and sync functions just call<br>\n  them.</p>\n</li>\n<li>\n<p>Fibers are not required for operations such as a GC, growing memory,<br>\n  etc.</p>\n</li>\n</ul>\n<p>The general idea is that the source of truth for the implementation of<br>\nWasmtime internals are all <code>async</code> functions. These functions are<br>\ncallable from synchronous functions in the API with a documented panic<br>\ncondition about avoiding them when <code>Config::async_support</code> is disabled.<br>\nWhen <code>async_support</code> is disabled it's known internally there should<br>\nnever be an <code>.await</code> point meaning that we can poll the future of the<br>\nasync version and assert that it's ready.</p>\n<p>This commit is not the full realization of plumbing <code>async</code> everywhere<br>\ninternally in Wasmtime. Instead all this does is plumb the async-ness of<br>\n<code>ResourceLimiterAsync</code> and that's it, aka memory and table growth are<br>\nnow properly async. It turns out though that these limiters are<br>\nextremely deep within Wasmtime and thus necessitated many changes to get<br>\nthis all working. In the end this ended up covering some of the trickier<br>\nparts of dealing with async and propagating that throughout the runtime.</p>\n<p>Most changes in this commit are intended to be straightforward, but a<br>\nsummary is:</p>\n<ul>\n<li>\n<p>Many more functions are <code>async</code> and <code>.await</code> their internals.</p>\n</li>\n<li>\n<p>Some instances of run-a-closure-and-catch-the-error are now replaced<br>\n  with type-with-<code>Drop</code> as that's the equivalent in the async world.</p>\n</li>\n<li>\n<p>Internal traits in Wasmtime are now <code>#[async_trait]</code> to be object<br>\n  safe. This has a performance impact detailed more below.</p>\n</li>\n<li>\n<p><code>vm::assert_ready</code> is used in synchronous contexts to assert that the<br>\n  async version is done immediately. This is intended to always be<br>\n  accompanied with an assert about <code>async_support</code> nearby.</p>\n</li>\n<li>\n<p><code>vm::one_poll</code> is used test if an asynchronous computation is ready<br>\n  yet and is used in a few locations where a synchronous public API says<br>\n  it'll work in <code>async_support</code> mode but fails with an async resource limiter.</p>\n</li>\n<li>\n<p>GC and other internals were simplified where <code>async</code> functions are now<br>\n  the \"guts\" and sync functions are thin veneers over these <code>async</code> functions.</p>\n</li>\n<li>\n<p>An example of new async functions are that lazy GC store allocation<br>\n  and instance allocation are both async functions now.</p>\n</li>\n<li>\n<p>In a small number of locations a conditional check of<br>\n<code>store.async_support()</code> is done. For example during GC if<br>\n<code>async_support</code> is enabled arbitrary yield points are injected. For<br>\n  libcalls if it's enabled <code>block_on</code> is used or otherwise it's asserted<br>\n  to complete synchronously.</p>\n</li>\n<li>\n<p>Previously <code>unsafe</code> functions dealing requiring external fiber<br>\n  handling are now all safe and <code>async</code>.</p>\n</li>\n<li>\n<p>Libcalls have a <code>block_on!</code> helper macro which should be itself a<br>\n  function-taking-async-closure but requires future Rust features to<br>\n  make it a function.</p>\n</li>\n</ul>\n<p>A consequence of this refactoring is that instantiation is now slower<br>\nthan before. For example from our <code>instantiation.rs</code> benchmark:</p>\n<div class=\"codehilite\" data-code-language=\"Rust\"><pre><span></span><code><span class=\"n\">sequential</span><span class=\"o\">/</span><span class=\"n\">pooling</span><span class=\"o\">/</span><span class=\"n\">spidermonkey</span><span class=\"p\">.</span><span class=\"n\">wasm</span>\n<span class=\"w\">                        </span><span class=\"n\">time</span><span class=\"p\">:</span><span class=\"w\">   </span><span class=\"p\">[</span><span class=\"mf\">2.6674</span><span class=\"w\"> </span><span class=\"err\">µ</span><span class=\"n\">s</span><span class=\"w\"> </span><span class=\"mf\">2.6691</span><span class=\"w\"> </span><span class=\"err\">µ</span><span class=\"n\">s</span><span class=\"w\"> </span><span class=\"mf\">2.6718</span><span class=\"w\"> </span><span class=\"err\">µ</span><span class=\"n\">s</span><span class=\"p\">]</span>\n<span class=\"w\">                        </span><span class=\"n\">change</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"o\">+</span><span class=\"mf\">20.975</span><span class=\"o\">%</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"mf\">21.039</span><span class=\"o\">%</span><span class=\"w\"> </span><span class=\"o\">+</span><span class=\"mf\">21.111</span><span class=\"o\">%</span><span class=\"p\">]</span><span class=\"w\"> </span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"w\"> </span><span class=\"o\">=</span><span class=\"w\"> </span><span class=\"mf\">0.00</span><span class=\"w\"> </span><span class=\"o\">&lt;</span><span class=\"w\"> </span><span class=\"mf\">0.05</span><span class=\"p\">)</span>\n<span class=\"w\">                        </span><span class=\"n\">Performance</span><span class=\"w\"> </span><span class=\"n\">has</span><span class=\"w\"> </span><span class=\"n\">regressed</span><span class=\"p\">.</span>\n</code></pre></div>\n<p>Other benchmarks I've been looking at locally in <code>instantiation.rs</code> have<br>\npretty wild swings from either a performance improvement in this PR of<br>\n10% to a regression of 20%. This benchmark in particular though, also<br>\none of the more interesting ones, is consistently 20% slower with this<br>\ncommit. Attempting to bottom out this performance difference it looks<br>\nlike it's largely \"just async state machines vs not\" where nothing else<br>\nreally jumps out in the profile to me. In terms of absolute numbers the<br>\ntime-to-instantiate is still in the single-digit-microsecond range with<br>\n<code>madvise</code> being the dominant function.</p>\n</blockquote>",
        "id": 534281773,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755109483
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3185022804\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>I'm starting this as a draft for now while I sort out CI things, but I also want to have some discussion about this ideally before landing. I plan on bringing this up in tomorrow's Wasmtime meeting.</p>\n</blockquote>",
        "id": 534281826,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755109512
    },
    {
        "content": "<p>alexcrichton updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>.</p>",
        "id": 534282555,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755109805
    },
    {
        "content": "<p>alexcrichton updated <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>.</p>",
        "id": 534291534,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755113453
    },
    {
        "content": "<p>tschneidereit <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3187547073\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<blockquote>\n<p>Attempting to bottom out this performance difference it looks<br>\nlike it's largely \"just async state machines vs not\" where nothing else<br>\nreally jumps out in the profile to me.</p>\n</blockquote>\n<p>IIUC, that means the overhead is a fixed cost that should be stable across different module types, as opposed to somehow scaling with _something_ about the module type itself? If so, that seems not ideal but okay to me, given that we're talking about 0.5us. Otherwise I'd like to understand the implications a bit more.</p>\n</blockquote>",
        "id": 534436793,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755161487
    },
    {
        "content": "<p>fitzgen <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3189433042\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>For posterity, in today's Wasmtime meeting, we discussed this PR and ways to claw back some of the perf regression. The primary option we discussed was using Cranelift to compile a state-initialization function, which we have on file as <a href=\"https://github.com/bytecodealliance/wasmtime/issues/2639\">https://github.com/bytecodealliance/wasmtime/issues/2639</a></p>\n</blockquote>",
        "id": 534536232,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755195222
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3192839411\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>Ok I've done some more performance profiling an analysis of this. After more thinking and more optimizing, I think I've got an idea for a design that is cheaper at runtime as well as doesn't require <code>T: Send</code>. It'll require preparatory refactorings though so I'm going to start things out in <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11442\">https://github.com/bytecodealliance/wasmtime/pull/11442</a> and we can go from there. I've got everything queued up in my head I think but it'll take some time to get it all into PRs. The other benefit of all of this is that it's going to resolve a number of issues related to unsafe code and unnecessary <code>unsafe</code>, e.g. #11442 handles an outstanding unsafe block in <code>table.rs</code>.</p>\n</blockquote>",
        "id": 534716891,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755294483
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3202685032\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>Further work/investigation on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11468\">https://github.com/bytecodealliance/wasmtime/pull/11468</a> revealed an optimization opportunity I was not aware of, but makes sense in retrospect: in an <code>async</code> function if an <code>.await</code> point is dynamically not executed then the function will execute faster. This makes sense to me because it avoids updating a state machine and/or spilling locals and execution continues as \"normal\", so hot-path/fast-path optimizations need to model, statically, that <code>.await</code> isn't necessary.</p>\n<p>With <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11468\">https://github.com/bytecodealliance/wasmtime/pull/11468</a> there's no performance regression currently. That's not the complete story but I'm growing confident we can land this PR without <code>T: Send</code> and without a performance regression. Basically we get to have our cake and eat it too.</p>\n</blockquote>",
        "id": 535213744,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755644399
    },
    {
        "content": "<p>alexcrichton closed without merge <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>.</p>",
        "id": 535412080,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755738917
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3208604235\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>Ok through all the various PRs above this PR is now entirely obsolete. All the benefits of this are on <code>main</code>, yay!</p>\n<p>There's a 5% performance regression on <code>main</code> relative to when I started this work which is due to <code>#[async_trait]</code> making boxed futures. Otherwise though I think it all worked out well!</p>\n</blockquote>",
        "id": 535412083,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755738918
    },
    {
        "content": "<p>tschneidereit <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3209497954\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<blockquote>\n<p>There's a 5% performance regression on <code>main</code> relative to when I started this work which is due to <code>#[async_trait]</code> making boxed futures.</p>\n</blockquote>\n<p>Can you say more about what kinds of things regressed? Or is this just \"everything is pretty uniformly 5% slower\"?</p>\n<p>And separately, is there anything we can do to claw this back? And if so, can we track that somewhere?</p>\n</blockquote>",
        "id": 535451988,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755764112
    },
    {
        "content": "<p>alexcrichton <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3211812244\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>Throughout this work I was watching the <code>sequential/pooling/(spidermonkey|wasi).wasm</code> benchmark defined in <code>benches/instantiation.rs</code> in this repo. I copied <code>spidermonkey.wasm</code> from Sightglass and otherwise this benchmark repeatedly instantiates in a loop these wasm modules. The 5% regression was time-to-instantiate-and-tear-down-the-store as measured by Criterion. Numbers were in the ~2us range for both modules and the 5% regression was on that number as well.</p>\n<p><a href=\"https://github.com/bytecodealliance/wasmtime/pull/11470\">https://github.com/bytecodealliance/wasmtime/pull/11470</a> was the cause of this change and in profiling and analyzing that my conclusion was it's more-or-less entirely due to <code>#[async_trait]</code>. Previously where we had only dynamic dispatch we now have dynamic dispatch plus heap-allocated futures. The extra heap allocation was what was showing up in the profile primarily different from before. Effectively each table and memory being allocated now requires a heap-allocated future to track the state of progressing through the allocation there.</p>\n<p>I don't really know of a great way to claw back this performance easily. One option is to way for <code>dyn</code>-compatible async traits in Rust, but that's likely to take awhile. Another option is to possibly have both an async and a sync trait method and we dynamically select which one depending on the resource limiter that's been configured. For the small wins here though I'd say that's probably not worth it, personally. Given the scale of the numbers here and the micro-benchmark nature I also wasn't planning on tracking this since we generally just try to get instantiation as fast as possible as opposed to \"must be below this threshold at all times\". In that sense it's a larger constant-factor than before, but that's naturally going to fluctuate over time IMO</p>\n</blockquote>",
        "id": 535565241,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755804216
    },
    {
        "content": "<p>tschneidereit <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430#issuecomment-3211938814\">commented</a> on <a href=\"https://github.com/bytecodealliance/wasmtime/pull/11430\">PR #11430</a>:</p>\n<blockquote>\n<p>Thank you, that's very helpful. I was mildly concerned because I thought you were talking about _everything_ being 5% slower. If it's just instantiation (and I now remember you mentioning this earlier), not e.g. execution throughput, then that's much less concerning. I think that all seems fine, then.</p>\n</blockquote>",
        "id": 535571842,
        "sender_full_name": "Wasmtime GitHub notifications bot",
        "timestamp": 1755807088
    }
]